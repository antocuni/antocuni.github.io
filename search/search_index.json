{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/","title":"About me","text":"<p> I am an Italy-based consultant with years of experience with both big brands and small business. I care about building solid and extensible designs, and about writing good and readable code. Being a developer, I like to write tools targeted to myself and to the others developers, to make our life easier. I hold a Ph.D. and a Master Degree in Computer Science.</p>"},{"location":"about/#area-of-expertise-and-foss-activity","title":"Area of expertise and FOSS activity","text":"<p>I am primarly a Python and PyPy expert, although I have experience with lots of other technologies, including .NET, Java, Android.</p> <p>Notable projects:</p> <ul> <li>Creator of SPy</li> <li>Co-founder of HPy</li> <li>Core developer of PyPy for 15+ years.</li> <li>Original creator of <code>pdb++</code> (now     maintained by others)</li> </ul> <p>Moreover,I have also been very active in the Python community for years, giving talks at various conferences such as EuroPython, PyCon Italia, PyCon US and more.</p>"},{"location":"about/#methodology","title":"Methodology","text":"<p>Thanks to my experience with PyPy, I have a strong and positive experience w.r.t. distributed, agile, sprint-driven and test-driven development, including good communication skills through IRC and emails, the ability to work from home and to be self-motivated to accomplish my goals.</p>"},{"location":"about/#contact","title":"Contact","text":"<p>Feel free send me an email.</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:cli","title":"cli","text":"<ul> <li>            First pypy-cli-jit benchmarks          </li> </ul>"},{"location":"tags/#tag:cpyext","title":"cpyext","text":"<ul> <li>            (Cape of) Good Hope for PyPy          </li> <li>            Inside cpyext: Why emulating CPython C API is so Hard          </li> </ul>"},{"location":"tags/#tag:cpython","title":"cpython","text":"<ul> <li>            Over the clouds: CPython, Pyodide and SPy          </li> </ul>"},{"location":"tags/#tag:gc","title":"gc","text":"<ul> <li>            PyPy for low-latency systems          </li> </ul>"},{"location":"tags/#tag:hpy","title":"hpy","text":"<ul> <li>            HPy @ Python Language Summit          </li> <li>            HPy kick-off sprint report          </li> <li>            Hello, HPy          </li> <li>            hpy 0.0.2: First public release          </li> </ul>"},{"location":"tags/#tag:jit","title":"jit","text":"<ul> <li>            First pypy-cli-jit benchmarks          </li> <li>            How to make your code 80 times faster          </li> </ul>"},{"location":"tags/#tag:profiling","title":"profiling","text":"<ul> <li>            (Cape of) Good Hope for PyPy          </li> <li>            How to make your code 80 times faster          </li> <li>            Inside cpyext: Why emulating CPython C API is so Hard          </li> </ul>"},{"location":"tags/#tag:pyodide","title":"pyodide","text":"<ul> <li>            Over the clouds: CPython, Pyodide and SPy          </li> </ul>"},{"location":"tags/#tag:pypy","title":"pypy","text":"<ul> <li>            (Cape of) Good Hope for PyPy          </li> <li>            A snake which bites its tail: PyPy JITting itself          </li> <li>            Binary wheels for PyPy          </li> <li>            EuroPython 2010 Videos available          </li> <li>            EuroPython 2010 report          </li> <li>            First pypy-cli-jit benchmarks          </li> <li>            Gothenburg sprint report          </li> <li>            How to ignore the annoying Cython warnings in PyPy 6.0          </li> <li>            How to make your code 80 times faster          </li> <li>            Improve .NET Integration          </li> <li>            Improving Memory Behaviour to Make Self-Hosted PyPy Translations Practical          </li> <li>            Inside cpyext: Why emulating CPython C API is so Hard          </li> <li>            Next sprint: Vilnius/Post EuroPython, 10-12th of July          </li> <li>            Pdb++ and rlcompleter_ng          </li> <li>            PhD Thesis about PyPy's CLI JIT Backend          </li> <li>            Porting the JIT to CLI (part 1)          </li> <li>            Porting the JIT to CLI (part 2)          </li> <li>            Porting the JIT to CLI (part 3)          </li> <li>            Progresses on the CLI JIT backend front          </li> <li>            Py3k status update          </li> <li>            Py3k status update #2          </li> <li>            Py3k status update #3          </li> <li>            Py3k status update #4          </li> <li>            Py3k status update #5          </li> <li>            Py3k status update #6          </li> <li>            PyPy Genova-Pegli Post-EuroPython Sprint June 27 - July 2 2011          </li> <li>            PyPy code swarm          </li> <li>            PyPy for low-latency systems          </li> <li>            PyPy migrates to Mercurial          </li> <li>            PyPy v7.0.0: triple release of 2.7, 3.5 and 3.6-alpha          </li> <li>            PyPy wants you!          </li> <li>            PyPy.NET goes Windows Forms          </li> <li>            Realtime image processing in Python          </li> <li>            The peace of green          </li> <li>            Trying to get PyPy to run on Python 3.0          </li> <li>            Using Tkinter and IDLE with PyPy          </li> <li>            Using virtualenv with PyPy          </li> </ul>"},{"location":"tags/#tag:pypy3","title":"pypy3","text":"<ul> <li>            Py3k status update          </li> <li>            Py3k status update #2          </li> <li>            Py3k status update #3          </li> <li>            Py3k status update #4          </li> <li>            Py3k status update #5          </li> <li>            Py3k status update #6          </li> </ul>"},{"location":"tags/#tag:release","title":"release","text":"<ul> <li>            PyPy v7.0.0: triple release of 2.7, 3.5 and 3.6-alpha          </li> </ul>"},{"location":"tags/#tag:speed","title":"speed","text":"<ul> <li>            (Cape of) Good Hope for PyPy          </li> <li>            How to make your code 80 times faster          </li> <li>            Inside cpyext: Why emulating CPython C API is so Hard          </li> </ul>"},{"location":"tags/#tag:sponsors","title":"sponsors","text":"<ul> <li>            PyPy for low-latency systems          </li> </ul>"},{"location":"tags/#tag:sprint","title":"sprint","text":"<ul> <li>            (Cape of) Good Hope for PyPy          </li> </ul>"},{"location":"tags/#tag:spy","title":"spy","text":"<ul> <li>            Over the clouds: CPython, Pyodide and SPy          </li> </ul>"},{"location":"tags/#tag:unicode","title":"unicode","text":"<ul> <li>            (Cape of) Good Hope for PyPy          </li> </ul>"},{"location":"2008/01/19/improve-net-integration/","title":"Improve .NET Integration","text":"<p>Originally published on the PyPy blog.</p> <p>A while ago Amit Regmi, a student from Canada, started working on the clr module improvements branch as a university project. </p> <p>During the sprint Carl Friedrich, Paul and me worked more on it and brought it to a mergeable state.</p> <p>It adds a lot of new features to the clr module, which is the module that allows integration between pypy-cli (aka PyPy.NET) and the surrounding .NET environment:</p> <ul> <li>full support to generic classes;</li> <li>a new importer hook, allowing things like from System import Math and so on;</li> <li>.NET classes that implements IEnumerator are treated as Python iterators; e.g. it's is possile to iterate over them with a for loop.</li> </ul> <p>This is an example of a pypy-cli session:</p> <pre>\n&gt;&gt;&gt;&gt; from System import Math\n&gt;&gt;&gt;&gt; Math.Abs(-42)\n42\n&gt;&gt;&gt;&gt; from System.Collections.Generic import List\n&gt;&gt;&gt;&gt; mylist = List[int]()\n&gt;&gt;&gt;&gt; mylist.Add(42)\n&gt;&gt;&gt;&gt; mylist.Add(43)\n&gt;&gt;&gt;&gt; mylist.Add(\"foo\")\nTraceback (most recent call last):\n  File \"&lt;console&gt;\", line 1, in &lt;interactive&gt;\nTypeError: No overloads for Add could match\n&gt;&gt;&gt;&gt; mylist[0]\n42\n&gt;&gt;&gt;&gt; for item in mylist: print item\n42\n43\n</pre> <p>This is still to be considered an alpha version; there are few known bugs and probably a lot of unknown ones :-), so don't expect it to work in every occasion. Still, it's a considerable step towards real world :-).</p>","tags":["pypy"]},{"location":"2008/01/19/pypynet-goes-windows-forms/","title":"PyPy.NET goes Windows Forms","text":"<p>Originally published on the PyPy blog.</p> <p>After having spent the last few days on understanding PyPy's JIT, today I went back hacking the clr module.  As a result, it is now possible to import and use external assemblies from pypy-cli, including Windows Forms </p> <p>Here is a screenshot of the result you get by typing the following at the pypy-cli interactive prompt:</p> <pre>\n&gt;&gt;&gt;&gt; import clr\n&gt;&gt;&gt;&gt; clr.AddReferenceByPartialName(\"System.Windows.Forms\")\n&gt;&gt;&gt;&gt; clr.AddReferenceByPartialName(\"System.Drawing\")\n&gt;&gt;&gt;&gt; from System.Windows.Forms import Application, Form, Label\n&gt;&gt;&gt;&gt; from System.Drawing import Point\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; frm = Form()\n&gt;&gt;&gt;&gt; frm.Text = \"The first pypy-cli Windows Forms app ever\"\n&gt;&gt;&gt;&gt; lbl = Label()\n&gt;&gt;&gt;&gt; lbl.Text = \"Hello World!\"\n&gt;&gt;&gt;&gt; lbl.AutoSize = True\n&gt;&gt;&gt;&gt; lbl.Location = Point(100, 100)\n&gt;&gt;&gt;&gt; frm.Controls.Add(lbl)\n&gt;&gt;&gt;&gt; Application.Run(frm)\n</pre> <p>Unfortunately at the moment you can't do much more than this, because we still miss support for delegates and so it's not possibile to handle events. Still, it's a step in the right direction :-).</p>","tags":["pypy"]},{"location":"2008/04/01/trying-to-get-pypy-to-run-on-python-30/","title":"Trying to get PyPy to run on Python 3.0","text":"<p>Originally published on the PyPy blog.</p> <p>As you surely know, Python 3.0 is coming; recently, they released Python 3.0 alpha 3, and the final version is expected around September. </p> <p>As suggested by the migration guide (in the PEP 3000), we started by applying 2to3 to our standard interpreter, which is written in RPython (though we should call it RPython 2.4 now, as opposed to RPython 3.0 -- see below).</p> <p>Converting was not seamless, but most of the resulting bugs were due to the new dict views, str/unicode changes and the missing \"reduce\" built-in. After forking and refactoring both our interpreter and the 2to3 script, the Python interpreter runs on Python 3.0 alpha 3!</p> <p>Next step was to run 2to3 over the whole translation toolchain, i.e. the part of PyPy which takes care of analyzing the interpreter in order to produce efficient executables; after the good results we got with the standard interpreter, we were confident that it would have been relatively easy to run 2to3 over it: unfortunately, it was not :-(.</p> <p>After letting 2to3 run for days and days uninterrupted, we decided to kill it: we assume that the toolchain is simply too complex to be converted in a reasonable amount of time.</p> <p>So, we needed to think something else; THE great idea we had was to turn everything upside-down: if we can't port PyPy to Py3k, we can always port Py3k to PyPy!</p> <p>Under the hood, the 2to3 conversion tool operates as a graph transformer: it takes the graph of your program (in the form of Python 2.x source file) and returns a transformed graph of the same program (in the form of Python 3.0 source file).  Since the entire translation toolchain of PyPy is based on graph transformations, we could reuse it to modify the behaviour of the 2to3 tool.  We wrote a general graph-inverter algorithm which, as the name suggests, takes a graph transformation and build the inverse transformation; then, we applied the graph inverter to 2to3, getting something that we called 3to2: it is important to underline that 3to2 was built by automatically analysing 2to3 and reversing its operation with only the help of a few manual hints. For this reason and because we are not keeping generated files under version control, we do not need to maintain this new tool in the Subversion repository.</p> <p>Once we built 3to2, it was relatively easy to pipe its result to our interpreter, getting something that can run Python 3.0 programs.</p> <p>Performance-wise, this approach has the problem of being slower at import time, because it needs to run (automatically) 3to2 every time the source is modified; in the future, we plan to apply our JIT techniques also to this part of the interpreter, trying to mitigate the slowdown until it is not noticeable anymore to the final user.</p> <p>In the next weeks, we will work on the transformation (and probably publish the technique as a research paper, with a title like \"Automatic Program Reversion on Intermediate Languages\").</p> <p>UPDATE: In case anybody didn't guess or didn't spot the acronym: The above was an April Fool's joke. Nearly nothing of it is true.</p>","tags":["pypy"]},{"location":"2008/05/28/progresses-on-the-cli-jit-backend-front/","title":"Progresses on the CLI JIT backend front","text":"<p>Originally published on the PyPy blog.</p> <p>In the last months, I've actively worked on the CLI backend for PyPy's JIT generator, whose goal is to automatically generate JIT compilers that produces .NET bytecode on the fly. </p> <p>The CLI JIT backend is far from be completed and there is still a lot of work to be done before it can handle the full PyPy's Python interpreter; nevertheless, yesterday I finally got the first .NET executable that contains a JIT for a very simple toy language called tlr, which implements an interpreter for a minimal register based virtual machine with only 8 operations.</p> <p>To compile the tlr VM, follow these steps:</p> <ol> <li><p>get a fresh checkout of the oo-jit branch, i.e. the branch where the CLI JIT development goes on:</p> <pre>\n$ svn co https://codespeak.net/svn/pypy/branch/oo-jit\n</pre> </li> <li><p>go to the oo-jit/pypy/jit/tl directory, and compile the tlr VM with the CLI backend and JIT enabled:</p> <pre>\n$ cd oo-jit/pypy/jit/tl/\n$ ../../translator/goal/translate.py -b cli --jit --batch targettlr\n</pre> </li> </ol> <p>The goal of our test program is to compute the square of a given number; since the only operations supported by the VM are addition and negation, we compute the result by doing repetitive additions; I won't describe the exact meaning of all the tlr bytecodes here, as they are quite self-documenting:</p> <pre>\nALLOCATE,    3,   # make space for three registers\nMOV_A_R,     0,   # i = a\nMOV_A_R,     1,   # copy of 'a'\n\nSET_A,       0,\nMOV_A_R,     2,   # res = 0\n\n# 10:\nSET_A,       1,\nNEG_A,\nADD_R_TO_A,  0,\nMOV_A_R,     0,   # i--\n\nMOV_R_A,     2,\nADD_R_TO_A,  1,\nMOV_A_R,     2,   # res += a\n\nMOV_R_A,     0,\nJUMP_IF_A,  10,   # if i!=0: goto 10\n\nMOV_R_A,     2,\nRETURN_A          # return res\n</pre> <p>You can find the program also at the end of the tlr module; to get an assembled version of the bytecode, ready to be interpreted, run this command:</p> <pre>\n$ python tlr.py assemble &gt; square.tlr\n</pre> <p>Now, we are ready to execute the code through the tlr VM; if you are using Linux/Mono, you can simply execute the targettlr-cli script that has been created for you; however, if you use Windows, you have to manually fish the executable inside the targettlr-cli-data directory:</p> <pre>\n# Linux\n$ ./targettlr-cli square.tlr 16\n256\n\n# Windows\n&gt; targettlr-cli-data\\main.exe square.tlr 16\n256\n</pre> <p>Cool, our program computed the result correctly! But, how can we be sure that it really JIT compiled our code instead of interpreting it? To inspect the code that it's generated by our JIT compiler, we simply set the PYPYJITLOG environment variable to a filename, so that the JIT will create a .NET assembly containing all the code that has been generated by the JIT:</p> <pre>\n$ PYPYJITLOG=generated.dll ./targettlr-cli square.tlr 16\n256\n$ file generated.dll\ngenerated.dll: MS-DOS executable PE  for MS Windows (DLL) (console) Intel 80386 32-bit\n</pre> <p>Now, we can inspect the DLL with any IL disassembler, such as ilasm or monodis; here is an excerpt of the disassembled code, that shows how our square.tlr bytecode has been compiled to .NET bytecode:</p> <pre>\n.method public static  hidebysig default int32 invoke (object[] A_0, int32 A_1)  cil managed\n{\n    .maxstack 3\n    .locals init (int32 V_0, int32 V_1, int32 V_2, int32 V_3, int32 V_4, int32 V_5)\n\n    ldc.i4 -1\n    ldarg.1\n    add\n    stloc.1\n    ldc.i4 0\n    ldarg.1\n    add\n    stloc.2\n    IL_0010:  ldloc.1\n    ldc.i4.0\n    cgt.un\n    stloc.3\n    ldloc.3\n    brfalse IL_003b\n\n    ldc.i4 -1\n    ldloc.1\n    add\n    stloc.s 4\n    ldloc.2\n    ldarg.1\n    add\n    stloc.s 5\n    ldloc.s 5\n    stloc.2\n    ldloc.s 4\n    stloc.1\n    ldarg.1\n    starg 1\n\n    nop\n    nop\n    br IL_0010\n\n    IL_003b:  ldloc.2\n    stloc.0\n    br IL_0042\n\n    ldloc.0\n    ret\n}\n</pre> <p>If you know a bit IL, you can see that the code generated is not optimal, as there are some redundant operations like all those stloc/ldloc pairs; however, while not optimal, it is still quite good code, not much different to what you would get by writing the square algorithm directly in e.g. C#.</p> <p>As I said before, all of this is still work in progress and there is still much to be done. Stay tuned :-).</p>","tags":["pypy"]},{"location":"2008/06/16/next-sprint-vilniuspost-europython-10-12th-of-july/","title":"Next sprint: Vilnius/Post EuroPython, 10-12th of July","text":"<p>Originally published on the PyPy blog.</p> <p>As happened in the last years, there will be a PyPy sprint just after EuroPython.  The sprint will take place in the same hotel as the conference, from 10th to 12th of July. </p> <p>This is a fully public sprint: newcomers are welcome, and on the first day we will have a tutorial session for those new to PyPy development.</p> <p>Some of the topics we would like to work on:</p> <ul> <li>try out Python programs and fix them or fix PyPy or fix performance bottlenecks</li> <li>some JIT improvement work</li> <li>port the stackless transform to ootypesystem</li> </ul> <p>Of course, other topics are also welcome.</p> <p>For more information, see the full announcement.</p>","tags":["pypy"]},{"location":"2008/06/22/pdb-and-rlcompleter_ng/","title":"Pdb++ and rlcompleter_ng","text":"<p>Originally published on the PyPy blog.</p> <p>When hacking on PyPy, I spend a lot of time inside pdb; thus, I tried to create a more comfortable environment where I can pass my nights :-). </p> <p>As a result, I wrote two modules:</p> <ul> <li>pdb.py, which extends the default behaviour of pdb, by adding some commands and some fancy features such as syntax highlight and powerful tab completion; pdb.py is meant to be placed somewhere in your PYTHONPATH, in order to override the default version of pdb.py shipped with the stdlib;</li> <li>rlcompleter_ng.py, whose most important feature is the ability to show coloured completions depending on the type of the objects.</li> </ul> <p>To find more informations about those modules and how to install them, have a look at their docstrings.</p> <p>It's important to underline that these modules are not PyPy specific, and they work perfectly also on top of CPython.</p>","tags":["pypy"]},{"location":"2008/06/27/pypy-code-swarm/","title":"PyPy code swarm","text":"<p>Originally published on the PyPy blog.</p> <p>Following the great success of code_swarm, I recently produced a video that shows the commit history of the PyPy project. </p> <p>The video shows the commits under the dist/ and branch/ directories, which is where most of the development happens.</p> <p>In the first part of the video, you can see clearly our sprint based approach: the video starts in February 2003, when the first PyPy sprint took place in Hildesheim: after a lot of initial activity, few commits happened in the next two months, until the second PyPy sprint, which took place in Gothenburg in late May 2003; around the minute 0:15, you can see the high commit rate due to the sprint.</p> <p>The next two years follow more or less the same pattern: very high activity during sprints, followed by long pauses between them; the most interesting breaking point is located around the minute 01:55; it's January 2005, and when the EU project starts, the number of commits just explodes, as well as the number of people involved.</p> <p>I also particularly appreciated minute 03:08 aka March 22, 2006: it's the date of my first commit to dist/, and my nickname magically appears; but of course I'm biased :-).</p> <p>The soundtrack is NIN - Ghosts IV - 34: thanks to xoraxax for having added the music and uploaded the video.</p> PyPy Codeswarm from solse@trashymail.com on Vimeo.","tags":["pypy"]},{"location":"2008/11/04/porting-the-jit-to-cli-part-1/","title":"Porting the JIT to CLI (part 1)","text":"<p>Originally published on the PyPy blog.</p> <p>As the readers of this blog already know, I have been working on the CLI JIT backend for some months: last Friday, it reached an important milestone, as it is now able to produce huge speedups for a little dynamic language.  To know how huge the speedup is, read on :-). </p> <p>The goal of PyPy JIT generator is to take an interpreter and, with the help of few annotations, automatically generate a JIT compiler for it.  In this post, we will talk about the tlc virtual machine: while tlc it is just a toy language, it contains some features that make it an interesting target for our JIT generator.</p> The tlc virtual machine <p>tlc is executed by a stack based, dynamically typed virtual machine (for those who knows a bit about the Python VM: does it sound familiar? :-)).</p> <p>There are three types of objects: integers, nil, and cons cells (i.e. lisp-like pairs of objects).</p> <p>As the VM is very simple, it provides only few opcodes:</p> <ul> <li>opcodes to manipulate the stack, like PUSH, POP, etc.</li> <li>integer operations, like ADD, MUL, all the comparisons, etc.: these operations can only be applied to integers;</li> <li>list operations, like CONS, CAR, CDR: these operations can only be applied to lists;</li> <li>other operations, including jumps and conditional jumps.</li> </ul> <p>The VM is interesting for our purposes because it has a lot of similarities with Python (though on a smaller scale, of course):</p> <ol> <li>it has to do type-checks at runtime before doing most of the operations;</li> <li>every time you do an arithmetic operation, it has to unbox the operand, do the computation, and the box the result again.</li> </ol> <p>This means that even if you have a program which only uses integers, you are paying a lot of overhead.</p> <p>To know more about this toy VM, look at its source code: the interesting bits are the classes used to represent objects, and the interp_eval function, which contains the main loop of the virtual machine.  As you can see, the implementation is quite straightforward; all the hint calls you see are the special annotations needed by the JIT generator to produce better code.</p> Let's JIT it! <p>So, the whole point is to generate a JIT compiler from it, isn't it?</p> <p>First, checkout a fresh copy of the oo-jit branch:</p> <pre>\n$ svn co https://codespeak.net/svn/pypy/branch/oo-jit\n</pre> <p>Then, go to the oo-jit/pypy/jit/tl directory, and compile the tlc VM with the CLI backend and JIT enabled:</p> <pre>\n$ cd oo-jit/pypy/jit/tl/\n$ ../../translator/goal/translate.py -b cli --jit --batch targettlc\n...\nlot of texts\n...\n</pre> <p>If everything went OK, you now have a targettlc-cli executable, which accepts two arguments: the name of the file containing the tlc program we want to run, and an integer to be passed to it.</p> <p>Luckily, in the same directory we have a factorial.tlc file that contains the bytecode for a function that -- guess? -- computes the factorial of a given integer; let's try it:</p> <pre>\n$ ./targettlc-cli factorial.tlc 5\nNon jitted:    120 (0.009371 seconds)\nWarmup jitted: 120 (0.208954 seconds)\nWarmed jitted: 120 (0.000323999999999991 seconds)\n</pre> <p>Cool, it seems that the result was computed correcly :-). As you can see from the output, we ran the program three times:</p> <ol> <li>by plain interpretation, without any jitting;</li> <li>with the jit enabled: this run includes the time spent by doing the compilation itself, plus the time spent by running the produced code;</li> <li>again with the jit enabled, but this time the compilation has already been done, so we are actually measuring how good is the code we produced.</li> </ol> <p>So, it's time to run a benchmark: let's try to compute the factorial of a very big number; the result will be 0, because obviously after a while we overflow, but after all we are interested in the time spent, not in the result:</p> <pre>\n$ ./targettlc-cli factorial.tlc 5000000\nNon jitted:    0 (19.93247 seconds)\nWarmup jitted: 0 (0.293229999999998 seconds)\nWarmed jitted: 0 (0.0494239999999984 seconds)\n\n$ python -c 'print 19.93247/0.0494239999999984'\n403.295362577\n</pre> <p>And no, I didn't make any mistake in copying&amp;pasting: the jitted version is really 400 times faster that the non jitted one!</p> <p>Warning: my laptop seems to be not very well suited for benchmarks, as the results vary a lot from run to run; I've run the benchmarks a lot of times, and I got speedup factors up to 500 times, so your results may be different.</p> More benchmarks <p>It's also interesting to compare the result with a manual written C# version of the factorial, to see how good is code we produced; to get reasonable results, we need to compute a larger factorial, to let to code to run a bit more:</p> <pre>\n$ ./targettlc-cli --onlyjit factorial.tlc 100000000\nWarmup jitted: 0 (0.980856 seconds)\nWarmed jitted: 0 (0.769716 seconds)\n\n$ mono factorial.exe 100000000\nC#:            0 (0.153777 seconds)\n\n$ python -c 'print 0.769716/0.153777'\n5.00540392907\n</pre> <p>We know that the generated code is far from being optimal, but probably the factor of five is at least partially due to the fact that Mono's own JIT is optimized for C#-like code, and our code has a completely different shape.</p> <p>All the benchmarks above were run under Linux, with Mono 1.9.1.  Here are the results for the same benchmarks, but run with Microsoft CLR (on a different machine, so the absolute values are not comparable):</p> <pre>\n$ ./targettlc-cli factorial.tlc 5000000\nNon jitted:    0 (15,640625 seconds)\nWarmup jitted: 0 (0,4375 seconds)\nWarmed jitted: 0 (0,03125 seconds)\n\n$ python -c 'print 15.640625/0.03125'\n500.5\n\n$ ./targettlc-cli --onlyjit factorial.tlc 100000000\nWarmup jitted: 0 (0,90625 seconds)\nWarmed jitted: 0 (0,515625 seconds)\n\n$ ./factorial.exe 100000000\nC#:            0 (0,34375 seconds)\n\n$ python -c 'print 0.515625/0.34375'\n1.5\n</pre> <p>The results are even better than before; this is probably thanks to CLR's JIT, that does a better job than Mono when faced to something which is different than the usual C#-like code.</p> Conclusions (for now) <p>This is a very important result, because it proves that PyPy's approach to JIT compilers can be applied effectively also to OO virtual machines; the result is even better than what I expected, because when generating code for .NET we have much less freedom than when generating assembly code, and I had to play some tricks to work around some .NET limitations.</p> <p>Moreover, it worked at the first try :-). I tried to compile the tlc virtual machine as soon as all the related JIT tests were passing, and surprisingly everything worked just fine, even if it was the very first time I was trying to apply some features of the JIT to something bigger than a test: I think this is yet another prove that Test Driven Development just works!</p> <p>Even if this is a major milestone, the CLI JIT backend is not yet completed: as a consequence it can't still be used for the full PyPy, but all the hardest problems should have been solved now.</p> <p>Since a lot of readers asked for more technical details, especially about the JIT, I will try to soon write a second blog post explaining how the CLI backend works internally, with a brief look to the generated code to see how it looks like.</p>","tags":["pypy"]},{"location":"2008/11/07/porting-the-jit-to-cli-part-2/","title":"Porting the JIT to CLI (part 2)","text":"<p>Originally published on the PyPy blog.</p> <p>In my previous post, we saw that PyPy JIT generator can produce huge speedups when applied to the tlc toy language. </p> <p>In this post we will dive a bit into the internals of PyPy JIT, to see how it manages to do so. Note that this is a very high level overview of how the JIT works, and applies to all backends.  Then, in the third post of this series, we will look closer at the CLI JIT backend, seeing how it works around some .NET limitations and how the generated code looks like.</p> PyPy JIT for dummies <p>As you surely know, the key idea of PyPy is that we are too lazy to write a JIT of our own: so, instead of passing nights writing a JIT, we pass years coding a JIT generator that writes the JIT for us :-).</p> <p>I'm not going to explain how the JIT generator does its job, (perhaps this will be the subject of another blog post), but how the generated JIT works.</p> <p>There are values that, if known at compile-time (i.e., when the JIT compiler runs), let the JIT to produce very efficient code.  In a dynamic language, types are the primary example: for instance, suppose you are a compiler and you have to compile to following Python function:</p> <pre>\ndef mysum(a):\n  return a + 1\n</pre> <p>At compile time, you don't have any knowledge about the type of the parameter: it could be integer, float, an user defined object, etc.  In this situation, the only safe choice is to emit code which does the usual, slow, full lookup to know how to perform the operations.</p> <p>On the other hand, suppose that you knew in advance that the parameter is an integer: this time, you could emit code that exploits this extra knowledge, by performing directly a fast integer addition.</p> <p>The idea behind PyPy JIT is that if you don't have enough knowledge to generate efficient code, you stop compiling and wait until you know exactly what you need.  Concretely, you emit code that runs until the point where you stopped the compilation, then it triggers a special procedure that restarts the compiler.  This time the JIT compiler knows everything you need, because you can inspect the state of the running program.</p> <p>Let's see an example: the first time the JIT compiles mysum, it produces something like this pseudo-code:</p> <pre>\nPyObject mysum_compiled(PyObject a)\n{\n  Type a_type = a.GetType();\n  switch(a_type) {\n      default: continue_compilation(a_type, &lt;position&gt;);\n  }\n}\n</pre> <p>If you call mysum(41), the execution goes in the default branch of the switch, thus calling continue_compilation: its job is to restart the JIT compiler, which now can emit fast code because it knows the exact type of a; then, it modifies the original mysum_compiled function, in order to make it executing the newly generated code the next time it encounters an integer at that point:</p> <pre>\nPyObject mysum_compiled(PyObject a)\n{\n  Type a_type = a.GetType();\n  switch(a_type) {\n      PyInteger: return new PyInteger(a.value+1); // fast path!\n      default: continue_compilation(a_type, &lt;position&gt;);\n  }\n}\n</pre> <p>From now on, every time we call mysum with an integer argument, the JIT compiler is not called anymore and the fast path is directly executed; if we happen to call mysum with a float arguments, the switch goes again in the default branch, and the JIT compiler is started once more to produce fast code also for this case.  What happens in practice is that compile-time and runtime are continuously intermixed, until the switches are stable enough and the compiler is not needed anymore.</p> <p>In PyPy jargon, this kind of \"growable switch\" is called flexswitch, and it's one of the most important concept of our JIT generator.</p> Promotion <p>How can the JIT generator know which values are useful to know to generate efficient code and which aren't?  Unfortunately it can't, or at least our JIT generator is not smart enough at the moment.</p> <p>To get the best from it, the developers of the VM need to instruct it by annotating the variables on which we want the JIT to stop until it knows the actual values; this is done by using particular hints, called promote and promote_class; variables annotated with such hints are said to be promoted. If something is promoted, a flexswitch is used to gain information about it, as seen in the last section.</p> <p>For an example, let's look at an excerpt from main dispatch loop of the tlc virtual machine:</p> <pre>\nelif opcode == ADD:\n  a, b = stack.pop(), stack.pop()\n  hint(a, promote_class=True)\n  hint(b, promote_class=True)\n  stack.append(b.add(a))\n</pre> <p>This the implementation of the ADD opcode: first, it pops two values from the stack; then, it computes the result; finally, it push the result to the stack again.  In between, both the classes of a and b have been promoted: this means that when the JIT emits the code for b.add(a), it knows exactly what is happening: if it sees that both are instances of the IntObj class, it inlines the method call and emits a fast integer addition instead.</p> Virtuals <p>The other important concept of the JIT is the presence of virtual structures, virtual lists, and virtual dictionaries.  Again, I'm not going to explain in depth how they work, but only why they are so important for generating highly efficient code.</p> <p>The essence of virtuals is that you don't allocate objects until you really need to do it, e.g. because they are being passed as an argument to some external function.  Instead, we store all the informations we need as local variables; e.g., in the case of a virtual structure, we create as many local variables as the number of its fields: if the structure escapes the local scope, we force it to a real object, by allocating memory on the heap and initializing it after the current value of the local variables.</p> <p>This technique allows the JIT to avoid the allocation of many temporary objects that hold intermediate results; consider for example the following Python loop:</p> <pre>\nresult = 0\nfor i in range(N):\n  result += i\nreturn result\n</pre> <p>Without the JIT, at each iteration, a new int object is created and bound to the result variable, while the previous one is discarded and not needed anymore.  By combining virtuals and promotion, the JIT can emit code that does the whole computation locally, and allocates a real object only at the end, when it escapes from the local scope because it is returned from the function.</p> Putting it all together <p>This is, essentially, how PyPy's generated JITs work.  To summarize, our JITs emit multiple versions of each chunk of code: each version is specialized and optimized for one particular case.</p> <p>The cost of selecting the right specialization to use (through flexswitches) is almost always negligible compared to how much time you save by running the fast version instead of the more-general-but-slow one.  Moreover, each specialized version knows the exact shape of the objects it's dealing with, so they can be virtualized to make the generated code even more efficient.</p> <p>At the end, the actual code generation is done by one of the JIT backends: the backends exploit all the knowledge gathered by the previous steps to produce highly efficient code, but this will be the subject of the next blog post.</p>","tags":["pypy"]},{"location":"2008/12/07/porting-the-jit-to-cli-part-3/","title":"Porting the JIT to CLI (part 3)","text":"<p>Originally published on the PyPy blog.</p> <p>In my two previous posts, we talked about the PyPy JIT generator, seeing that it can produce huge speedups and how its backend-independent frontend works. </p> <p>In this post, we will look closer at the internals of the CLI JIT backend; in particular, we will see how we work around some serious limitations of the platform, and why these workarounds didn't have any serious impact on the performances of our toy virtual machine.</p> Graphs, blocks, links <p>One of the core aspect of PyPy translator is the concept of flow graph: a flow graph is a data structure that represents the code we are operating on. It is composed by a set of basic blocks, each block containing a sequence of operations; blocks are connected together by links, and each link can carry a variable number of arguments whose value is passed to the target block.  In case a block contains more than one outgoing links, the one to follow is selected by looking at the value of a designated variable (the exitswitch), thus making possible to implement conditional jumps.  To have a more complete description of the flow graphs model, check the documentation.</p> <p>As we saw in the previous post, the generated JIT compiler makes heavy use of flexswitches to generate efficient code, continuously intermixing JIT-compile time and runtime.</p> <p>In terms of graphs, we can think of a flexswitch as a special block whose links change over time.  In particular, adding a new case to the flexswitch is equivalent to create a link whose target is a new block where the just generated code starts.  Thus, the graphs grows over the time, as showed by the following images:</p> <p>In the images above, the block containing the flexswitch is colored in cyan. In the first picture, there is only one block connected to the flexswitch: this block contains the code to restart the JIT compilation.  The second picture shows the graph after the first case has been added: you can clearly see that a new block has been created and attached to the flexswitch. Finally, the third picture shows the graph after a while, with a lot of new blocks attached.</p> Translate graphs to CLI <p>Conceptually, the goal of the CLI JIT backend is to express these graphs in terms of CLI bytecode.</p> <p>Translating the single block is easy, as it is just a list of sequential operation, and it's straightforward to map each operation to the equivalent CLI opcode or to a call to a helper method.  Moreover, we need a way to express links between the various basic blocks: if the links are known in advance, render them is as easy as emitting a (potentially conditional) jump to the target block.  Thus, we won't discuss this part in detail, as it is quite straightforward.</p> <p>The hard part is how to implement flexswitches: at the time when we are emitting the code, some of the blocks of this growable graph don't even exist: how can we make a jump to a non existent block of code?  For backends that emit assembly code, it is rather easy: when they need to add a new case to the flexswitch, they can just patch the existing code to insert a jump to a newly allocated area of the memory, where the new code is being generated in.</p> <p>For CLI this approach is not feasible, as the VM will never allow us to modify existing code. Thus, we need to think of a different approach.</p> Graphs and methods <p>In .NET, the basic unit of compilation is the method: the only way to execute some bytecode is to wrap it into a method.  Moreover, it is not possible to execute a method until it has been completed, and after this point it is no longer possible to add new code.</p> <p>Because of all these constraints we cannot simply map each graph to its own method, since we saw that our graphs can grow after they have already been executed few times.</p> <p>Hence, we need to distinguish between the two concepts:</p> <ul> <li>a graph is the logical unit of code as seen by the JIT compiler: concretely, the CLI JIT backend renders it as one or more methods;</li> <li>a method is a collection of basic blocks; each method has the so called parent graph, i.e. the graph its blocks logically belongs to.</li> </ul> <p>The first method of a graph is called main method (which has nothing to do with the Main static methods found in .exe files); other methods are called children methods.</p> <p>When we want to add a new case to the flexswitch, we create a method containing all the new code; then we wrap the method inside a delegate (the .NET equivalent of a function pointer) and pass it to the flexswitch, so that it can later invoke it.</p> The hard bit: non-local links <p>Using this approach, after a while the blocks of our original graph are scattered over a lot of different methods; however, there are no constraints about how these blocks can be linked together, so it happens to have links between blocks which are not in the same method. In the following, we will refer to them as non-local links.</p> <p>If the non-local block we want to jump to happens to be at the beginning of its containing method, it is enough to invoke the method; but, what if we want to jump somewhere in the middle?  What we really want is to produce a method which has multiple entry-points; again, doing it in assembly would be trivial, but the virtual machine does not provide any support for it, so we need a work around.</p> <p>Each method in a graph is assigned an unique 16 bit method id; each block in a method is assigned a progressive 16 bit block number.  From this two numbers, we can compute the block id as an unsigned integer, by storing the method id in the first 16 bits and the block number in the second 16 bits. By construction, the block id is guaranteed to be unique in the graph.</p> <p>The following picture shows a graph composed of three methods; the id of each method is shown in red, while the block ids are shown in red (for the method id part) and black (for the block number part).  The graph contains three non-local links; in particular, note the link between blocks 0x00020001 and 0x00010001 which connects two block that resides in different methods.</p> <p>Every method contains a special dispatch block, (not shown in the picture above) whose goal is to jump to the specified block number inside the method itself.  The first argument of a child method is always a block id; when the method starts, it immediately jumps to the dispatch block, and thus to the desired block.</p><p>For example, suppose to have a method which contains 3 blocks numbered 0, 1, 2; here is how its dispatch blocks looks like; for simplicity it is shown as C# code, but it is actually generated as IL bytecode:</p> <pre>\n// dispatch block\nint methodid = (blockid &amp; 0xFFFF0000) &gt;&gt; 16); // take the first 16 bits\nint blocknum = blockid &amp;&amp; 0x0000FFFF;         // take the second 16 bits\n\nif (methodid != MY_METHOD_ID) {\n// jump_to_unknown block\n...\n}\n\nswitch(blocknum) {\ncase 0:\ngoto block0;\ncase 1:\ngoto block1;\ncase 2:\ngoto block2;\ndefault:\nthrow new Exception(\"Invalid block id\");\n}\n</pre> <p>Whenever we want to jump to a non-local block, it is enough to store the block id in the appropriate variable and jump to the dispatch block.  If the block resides in a different method, the jump_to_unknown block is entered; this special block is implemented differently by the main method and the child methods, as we will see soon.</p> <p>Each time a new method is added to the graph, we build a delegate for it, and store it in a special array called method_map; since we assign the method id sequentially starting from 0, we are sure that to fetch the method whose id is n we can simply load the n-th element of the array.</p> <p>The jump_to_unknown block of the main method uses this array to select the right method, and calls it (FlexSwitchCase is the type of delegates for all children methods):</p> <pre>\n// jump_to_unknown block of the main method\nFlexSwitchCase meth = method_map[methodid];\nblockid = meth(blockid, ...); // execute the method\ngoto dispatch_block;\n</pre> <p>Each child method returns a block id specifying the next block to jump to; after its execution, we assign the return value to the blockid variable, and jump again to the dispatch block, which will jump again to the appropriate block.</p> <p>Keeping this in mind, it is straightforward to implement the jump_to_unknown block of children methods: it is enough to return the target block id to the caller, and let its dispatch loop do the right thing. If the caller is also a child method, it will return it again, until we reach the dispatch loop of the main method, which will finally do the jump.  In theory, we could implement things differently and jumping directly from a child method to another one, but in that case the call stack could grows indefinitely in case of a tight loop between two blocks residing in different methods.</p> <p>To implement the dispatch block we can exploit the switch opcode of the CLI; if the .NET JIT is smart enough, it can render it using an indirect jump; overall, jumping to a non-local block consists of an indirect function call (by invoking the delegate) plus an indirect jump (by executing the switch opcode); even if this is more costly than a simple direct jump, we will see in the next section that this not the main source of overhead when following a non-local link.</p> <p>Obviously, the slow dispatching logic is needed only when we want to jump to a non-local block; if the target block happens to reside in the same method as the current one, we can directly jump to it, completely removing the overhead.</p> <p>Moreover, the dispatch blocks are emitted only if needed, i.e. if the parent graph contains at least one flexswitch; graphs without flexswitches are rendered in the obvious way, by making one method per graph.</p> The slow bit: passing arguments <p>Jumping to the correct block is not enough to follow a link: as we said before, each link carries a set of arguments to be passed from the source to the target block.  As usual, passing arguments across local links is easy, as we can just use local variables to hold their values; on the other hand, non-local links make things more complex.</p> <p>The only way to jump to a block is to invoke its containing method, so the first solution that comes to mind is to specify its input arguments as parameter of the method; however, each block has potentially a different number (and different types) of input arguments than every other block, so we need to think of something else.</p> <p>An alternative solution could be to compute the union of the sets of input arguments of all the blocks in the method, and use this set as a signature for the method; this way, there would be enough space to specify the input arguments for every block we might want to jump to, each block ignoring the exceeding unused parameters.</p> <p>Unfortunately, all the children methods must have the very same signature, as they are all called from the same calling site in the dispatch block of the main method.  Since the union of the set of input arguments (and hence the computed signature) varies from method to method, this solution cannot work.</p> <p>We might think to determine the signature by computing the union of input arguments of all blocks in the graph; this way, all the children methods would have the same signature.  But as we said above, the graph grows new blocks at runtime, so we cannot determine in advance which set of input arguments we will need.</p> <p>To solve the problem we need a way to pass a variable number of arguments without knowing in advance neither their number nor their types.  Thus, we use an instance of this class:</p> <pre>\npublic class InputArgs {\npublic int[] ints;\npublic float[] floats;\npublic object[] objs;\n...\n}\n</pre> <p>Since the fields are arrays, they can grow as needed to contain any number of arguments; arguments whose type is primitive are stored in the ints or floats array, depending on their type; arguments whose type is a reference type are stored in the objs array: it's up to each block to cast each argument back to the needed type.</p> <p>This solution impose a huge overhead on both writing and reading arguments:</p> <ul> <li>when writing, we need to make sure that the arrays are big enough to contains all the arguments we need; if not, we need to allocate a bigger array.  Moreover, for each argument we store into the array the virtual machine performs a bound-check, even if we know the index will never be out of bounds (because we checked the size of the array in advance);</li> <li>when reading, the same bound-check is performed for each argument read; moreover, for each value read from the objs array we need to insert a downcast.</li> </ul> <p>To mitigate the performance drop, we avoid to allocate a new InputArgs object each time we do a non-local jump; instead, we preallocate one at the beginning of the main method, and reuse it all the time.</p> <p>Our benchmarks show that passing arguments in arrays is about 10 times slower than passing them as real parameter of a method.  Unfortunately, we couldn't come up with anything better.</p> Implement flexswitches <p>Now, we can exploit all this machinery to implement flexswitches, as this is our ultimate goal.  As described above, the point is to be able to add new cases at runtime, each case represented as a delegate.  Here is an excerpt of the C# class that implements a flexswitch that switches over an integer value:</p> <pre>\npublic class IntLowLevelFlexSwitch:\n{\npublic uint default_blockid = 0xFFFFFFFF;\npublic int numcases = 0;\npublic int[] values = new int[4];\npublic FlexSwitchCase[] cases = new FlexSwitchCase[4];\n\npublic void add_case(int value, FlexSwitchCase c)\n{\n...\n}\n\npublic uint execute(int value, InputArgs args)\n{\nfor(int i=0; i&lt;numcases; i++)\nif (values[i] == value) {\n return cases[i](0, args);\n}\nreturn default_blockid;\n}\n}\n</pre> <p>For each case, we store both the triggering value and the corresponding delegate; the add_case method takes care to append value and c to the values and cases arrays, respectively (and resize them if necessary).  The interesting bit is the execute method: it takes a value and a set of input arguments to be passed across the link and jumps to the right block by performing a linear search in the values array.</p> <p>As shown by previous sections, the first argument of a FlexSwitchCase is the block id to jump to; since when we go through a flexswitch we always want to jump to the first block of the method, we pass the special value 0 as a block id, which precisely means jump to the first block.  This little optimization let us not to have to explicitly store the block id for the first block of all the cases.</p> <p>The value returned by execute is the next block id to jump to; if the value is not found in the values array, we return the default_blockid, whose value has been set before by the JIT compiler; default_blockid usually points to a block containing code to restart the JIT compiler again; when the JIT compiler restarts, it emits more code for the missing case, then calls add_case on the flexswitch; from now on, the new blocks are wired into the existing graph, and we finally managed to implement growable graphs.</p> Performances <p>As we saw, implementing growable graphs for CLI is a pain, as the virtual machine offers very little support, so we need an incredible amount of workarounds. Moreover, the code generated is much worse than what an assembly backend could produce, and the cost of following a non-local link is very high compared to local links.</p> <p>However, our first blog post showed that we still get very good performances; how is it possible?</p> <p>As usual in computer science, most of the time of a running program in spent in a tiny fraction of the code; our benchmark is no exception, and the vast majority of the time is spent in the inner loop that multiplies numbers; the graph is built in such a way that all the blocks that are part of the inner loop reside in the same method, so that all links inside are local (and fast).</p> <p>Flexswitches and non-local links play a key role to select the right specialized implementation of the inner loop, but once it is selected they are not executed anymore until we have finished the computation.</p> <p>It is still unclear how things will look like when we will compile the full Python language instead of a toy one; depending on the code, it could be possible to have non-local links inside the inner loop, thus making performance much worse.</p> Alternative implementations <p>Before implementing the solution described here, we carefully studied a lot of possible alternatives, but all of them either didn't work because of a limitation of the virtual machine or they could work but with terrible performances.</p> <p>In particular, in theory it is possible to implement non-local links using tail calls, by putting each block in its own method and doing a tail call instead of a jump; this would also solve the problem of how to pass arguments, as each method could have its own signature matching the input args of the block.  I would like to explain this solution in a more detailed way as I think it's really elegant and nice, but since this post is already too long, I'll stop here :-).</p> <p>In theory, if the .NET JIT were smart enough it could inline and optimize away the tail calls (or at least many of those) and give us very efficient code. However, one benchmark I wrote shows that tail calls are up to 10 times slower (!!!) than normal calls, thus making impractical to use them for our purposes.</p> Conclusion <p>Despite the complexity of the implementation, our result are extremely good; the speedup we got is impressive, and it proves that PyPy's approach to JIT compiler can work well also on top of object oriented virtual machines like .NET or the JVM.</p> <p>Generating bytecode for those machine at runtime is not a new idea; Jython, IronPython, JRuby and other languages have been doing this for years. However, Jython and IronPython do only a simple \"static\" translation, which doesn't take advantage of the informations gathered at runtime to generate better, faster and specialized code.  Recently, JRuby grew a new strategy to JIT-compile only hotspots, taking advantage of some informations gathered while interpreting the code; this is still a \"one-shot\" compilation, where the compiled code does not change over time.</p> <p>To my knowledge, PyPy brings the first example of a language which implements a truly JIT compiler on top of the underlying JIT compiler of the virtual machine, emitting bytecode that changes and adapts over the time.  If someone knows other languages doing that, I would really like to know more.</p> <p>Being so innovative, the problem of this approach is that the current virtual machines are not designed to support it in a native way, and this forces us to put a lot of workarounds that slow down the generated code.  The hope is that in the future the virtual machines will grow features that help us to generate such kind of code.  The experimental Da Vinci VM seems to go in the right direction, so it is possible that in the future I will try to write a JIT backend for it.</p> <p>At the moment, the CLI JIT backend is almost complete, and all the hardest problems seems to be solved; the next step is to fix all the remaining bugs and implement some minor feature that it's still missing, then try to apply it to the full Python language and see what is the outcome.</p>","tags":["pypy"]},{"location":"2009/10/15/first-pypy-cli-jit-benchmarks/","title":"First pypy-cli-jit benchmarks","text":"<p>Originally published on the PyPy blog.</p> <p>As the readers of this blog already know, I've been working on porting the JIT to CLI/.NET for the last months.  Now that it's finally possible to get a working pypy-cli-jit, it's time to do some benchmarks. </p> <p>Warning: as usual, all of this has to be considered to be a alpha version: don't be surprised if you get a crash when trying to run pypy-cli-jit.  Of course, things are improving very quickly so it should become more and more stable as days pass.</p> <p>For this time, I decided to run four benchmarks. Note that for all of them we run the main function once in advance, to let the JIT recoginizing the hot loops and emitting the corresponding code.  Thus, the results reported do not include the time spent by the JIT compiler itself, but give a good measure of how good is the code generated by the JIT.  At this point in time, I know that the CLI JIT backend spends way too much time compiling stuff, but this issue will be fixed soon.</p> <ul> <li>f1.py: this is the classic PyPy JIT benchmark. It is just a function that does some computational intensive work with integers.</li> <li>floatdemo.py: this is the same benchmark involving floating point numbers that have already been described in a previous blog post.</li> <li>oodemo.py: this is just a microbenchmark doing object oriented stuff such as method calls and attribute access.</li> <li>richards2.py: a modified version of the classic richards.py, with a warmup call before starting the real benchmark.</li> </ul> <p>The benchmarks were run on a Windows machine with an Intel Pentium Dual Core E5200 2.5GHz and 2GB RAM, both with .NET (CLR 2.0) and Mono 2.4.2.3.</p> <p>Because of a known mono bug, if you use a version older than 2.1 you need to pass the option -O=-branch to mono when running pypy-cli-jit, else it will just loop forever.</p> <p>For comparison, we also run the same benchmarks with IronPython 2.0.1 and IronPython 2.6rc1.  Note that IronPython 2.6rc1 does not work with mono.</p> <p>So, here are the results (expressed in seconds) with Microsoft CLR:</p> Benchmark pypy-cli-jit ipy 2.0.1 ipy 2.6 ipy2.01/ pypy ipy2.6/ pypy f1 0.028 0.145 0.136 5.18x 4.85x floatdemo 0.671 0.765 0.812 1.14x 1.21x oodemo 1.25 4.278 3.816 3.42x 3.05x richards2 1228 442 670 0.36x 0.54x <p>And with Mono:</p> Benchmark pypy-cli-jit ipy 2.0.1 ipy2.01/ pypy f1 0.042 0.695 16.54x floatdemo 0.781 1.218 1.55x oodemo 1.703 9.501 5.31x richards2 720 862 1.20x <p>These results are very interesting: under the CLR, we are between 5x faster and 3x slower than IronPython 2.0.1, and between 4.8x faster and 1.8x slower than IronPython 2.6.  On the other hand, on mono we are consistently faster than IronPython, up to 16x.  Also, it is also interesting to note that pypy-cli runs faster on CLR than mono for all benchmarks except richards2.</p> <p>I've not investigated yet, but I think that the culprit is the terrible behaviour of tail calls on CLR: as I already wrote in another blog post, tail calls are ~10x slower than normal calls on CLR, while being only ~2x slower than normal calls on mono.  richads2 is probably the benchmark that makes most use of tail calls, thus explaining why we have a much better result on mono than CLR.</p> <p>The next step is probably to find an alternative implementation that does not use tail calls: this probably will also improve the time spent by the JIT compiler itself, which is not reported in the numbers above but that so far it is surely too high to be acceptable. Stay tuned.</p>","tags":["jit","pypy","cli"]},{"location":"2010/07/26/europython-2010-report/","title":"EuroPython 2010 report","text":"<p>Originally published on the PyPy blog.</p> <p>So, EuroPython 2010 is over, I am flying home and it's time to write a report about the conference from the PyPy point of view. </p> <p>As usual, the conference was very interesting and went very well. The quality of the talks I attended to was high on average and most importantly I could meet a lot of interesting people to discuss various things.</p> <p>On the first day, Armin, Amaury and I presented the usual PyPy status talk (here are the slides): the talk is an extended version of the one that I and Armin presented at Pycon Italia in May and is divided in three parts: first I talked about the current status of the project, what is the content of the recent 1.2 and 1.3 releases and showed a demo of a simple Django application that renders a Mandelbrot fractal and is measurably faster on PyPy than on CPython.  In the second part of the talk, Armin gave an introduction about the ideas that stand behind the JIT.  Finally, in the third part Amaury explained how the new cpyext module lets PyPy to compile and load existing CPython extensions written in C.</p> <p>I think that the talk was well received: the only drawback is that there was no time to answer questions at the end of the presentation.  However, we received a lot of \"offline\" questions after the talk finished and thorough the whole conference: it is always great to see that people are interested in our work, and I'd like to thank everybody for the feedback that they gave to us.</p> <p>PyPy was also mentioned in the interesting Mark Shannon's talk, where he compared the optimization techniques used by PyPy, Unladen Swallow and HotPy, which is Mark's own PhD project.  Moreover, Henrik Vendelbo gave a talk about how to tweak PyPy to produce a standalone executable which embeds a whole python application to make deployment easier, while Andrew Francis explained his implementation of the Go select statement based on the stackless.py module implemented in PyPy.  Personally, I am glad to see that people start to think of PyPy as a useful starting point to experiment with new features and use cases that we did not think about: after all, one of PyPy explicit goals is to be \"flexible and easy to experiment with\".</p> <p>After the conference there were the usual post EuroPython sprints: this year we had not planned a PyPy sprint, but some people showed interest in it and since Armin and I happened to be still around the day after the conference, we decided to do a mini 1-day sprint, with 6 or 7 people present. Since there were only two core developers it was impossible to use our usual pairing scheme, in which every newcomer pairs with someone who is experienced with the source code to gain knowledge of it.  However, I think it was still a successful day of work, and we managed to fix a couple of bugs that was standing in our issue tracker.  Again, I'd like to thank all the people that came and worked with us during the sprint.</p> <p>In conclusion I really enjoyed the EuroPython 2010 experience: the fact that I managed to find a place in Birmingham where to eat a good Italian-style \"gelato\" helped a lot :-).</p>","tags":["pypy"]},{"location":"2010/08/17/europython-2010-videos-available/","title":"EuroPython 2010 Videos available","text":"<p>Originally published on the PyPy blog.</p> <p>Hi all, </p> <p>the videos of the talks from EuroPython 2010 are now available on blip.tv: in particular, there are the three videos of the PyPy talk.</p> <p>Part 1: What's news in PyPy 1.2 and 1.3 (by Antonio Cuni)</p> <p>Part 2: Just in Time compilation (by Armin Rigo)</p> <p>Part 3: cpyext (by Amaury Forgeot d'Arc)</p> <p>Moreover, here is Mark Shannon's talk which compares HotPy, Unladen Swallow and PyPy:</p>","tags":["pypy"]},{"location":"2010/08/02/using-virtualenv-with-pypy/","title":"Using virtualenv with PyPy","text":"<p>Originally published on the PyPy blog.</p> <p>Thanks to the work that was recently done on the sys-prefix branch, it is now possible to use virtualenv with PyPy. </p> <p>To try it, you need:</p> <ul> <li>a recent version of PyPy: PyPy 1.3 does not contain the necessary logic to work with virtualenv, so you need a more recent PyPy from subversion trunk. You can either build it by yourself or download one of our precompiled nightly builds</li> <li>a copy of virtualenv-pypy: this is a fork of virtualenv that contains all the patches needed to work with PyPy, and hopefully will be merged back at some point.  It should be totally compatible with the official version of virtualenv, so it is safe to use it even to create non-PyPy environments.  If you notice some weird behavior that does not happen with the standard virtualenv, please let us know.</li> </ul> <p>The directory layout has been redesigned in a way that it is possible to use virtualenv to install a PyPy both from a precompiled tarball or from an svn checkout:</p> <pre>\n# from a tarball\n$ virtualenv -p /opt/pypy-c-jit-76426-linux/bin/pypy my-pypy-env\n\n# from the svn checkout\n$ virtualenv -p /path/to/pypy-trunk/pypy/translator/goal/pypy-c my-pypy-env\n</pre> <p>Once the environment has been created, you can enter it as usual. Note that bin/python is now a symlink to bin/pypy.</p> <p>Enjoy it :-)</p>","tags":["pypy"]},{"location":"2010/10/25/the-peace-of-green/","title":"The peace of green","text":"<p>Originally published on the PyPy blog.</p> <p>No, we are not going to talk about the environment (i.e., the set of variables as printed by /usr/bin/env. What else? :-)). </p> <p>After months in which we had a couple of tests failing every day, we finally managed to turn (almost) everything green today, at least on Linux.  Enjoy this screenshoot taken from the nightly build page:</p> <p>As usual, the full buildbot results can be seen from the summary page.</p> <p>cheers, Anto</p>","tags":["pypy"]},{"location":"2010/10/22/phd-thesis-about-pypys-cli-jit-backend/","title":"PhD Thesis about PyPy's CLI JIT Backend","text":"<p>Originally published on the PyPy blog.</p> <p>Hi all, </p> <p>few months ago I finished the PhD studies and now my thesis is available, just in case someone does not have anything better to do than read it :-).</p> <p>The title of the thesis is High performance implementation of Python for CLI/.NET with JIT compiler generation for dynamic languages, and its mainly based on my work on the CLI backend for the PyPy JIT (note that the CLI JIT backend is currently broken on trunk, but it's still working in the cli-jit branch).</p> <p>The thesis might be useful also for people that are not directly interested in the CLI JIT backend, as it also contains general information about the inner workings of PyPy which are independent from the backend: in particular, chapters 5 and 6 explain how the JIT frontend works.</p> Here is the summary of chapters: <ol> <li>Introduction</li> <li>The problem</li> <li>Enter PyPy</li> <li>Characterization of the target platform</li> <li>Tracing JITs in a nutshell</li> <li>The PyPy JIT compiler generator</li> <li>The CLI JIT backend</li> <li>Benchmarks</li> <li>Conclusion and Future Work</li> </ol> <p>cheers, Anto</p>","tags":["pypy"]},{"location":"2010/11/26/improving-memory-behaviour-to-make-self-hosted-pypy-translations-practical/","title":"Improving Memory Behaviour to Make Self-Hosted PyPy Translations Practical","text":"<p>Originally published on the PyPy blog.</p> <p>In our previous blog post, we talked about how fast PyPy can translate itself compared to CPython.  However, the price to pay for the 2x speedup was an huge amount of memory: actually, it was so huge that a standard -Ojit compilation could not be completed on 32-bit because it required more than the 4 GB of RAM that are addressable on that platform.  On 64-bit, it consumed 8.3 GB of RAM instead of the 2.3 GB needed by CPython. </p> <p>This behavior was mainly caused by the JIT, because at the time we wrote the blog post the generated assembler was kept alive forever, together with some big data structure needed to execute it.</p> <p>In the past two weeks Anto and Armin attacked the issue in the jit-free branch, which has been recently merged to trunk.  The branch solves several issues. The main idea of the branch is that if a loop has not been executed for a certain amount of time (controlled by the new loop_longevity JIT parameter) we consider it \"old\" and no longer needed, thus we deallocate it.</p> <p>(In the process of doing this, we also discovered and fixed an oversight in the implementation of generators, which led to generators being freed only very slowly.)</p> <p>To understand the freeing of loops some more, let's look at how many loops are actually created during a translation. The purple line in the following graph shows how many loops (and bridges) are alive at any point in time with an infinite longevity, which is equivalent to the situation we had before the jit-free branch.  By contrast, the blue line shows the number of loops that you get in the current trunk: the difference is evident, as now we never have more than 10000 loops alive, while previously we got up to about 37000 ones.  The time on the X axis is expressed in \"Giga Ticks\", where a tick is the value read out of the Time Stamp Counter of the CPU.</p> <p>The grey vertical bars represent the beginning of each phase of the translation:</p> <ul> <li>annotate performs control flow graph construction and type inference.</li> <li>rtype lowers the abstraction level of the control flow graphs with types to that of C.</li> <li>pyjitpl constructs the JIT.</li> <li>backendopt optimizes the control flow graphs.</li> <li>stackcheckinsertion finds the places in the call graph that can overflow the C stack and inserts checks that raise an exception instead.</li> <li>database_c produces a database of all the objects the C code will have to know about.</li> <li>source_c produces the C source code.</li> <li>compile_c calls the compiler to produce the executable.</li> </ul> <p>You can nicely see, how the number of alive graphs drops shortly after the beginning of a new phase.</p> <p>Those two fixes, freeing loops and generators, improve the memory usage greatly: now, translating PyPy on PyPy on 32-bit consumes 2 GB of RAM, while on CPython it consumes 1.1 GB. This result can even be improved somewhat, because we are not actually freeing the assembler code itself, but only the large data structures around it; we can consider it as a residual memory leak of around 150 MB in this case.  This will be fixed in the jit-free-asm branch.</p> <p>The following graph shows the memory usage in more detail:</p> <ul> <li>the blue line (cpython-scaled) shows the total amount of RAM that the OS allocates for CPython.  Note that the X axis (the time) has been scaled down so that it spans as much as the PyPy one, to ease the comparison. Actually, CPython took more than twice as much time as PyPy to complete the translation</li> <li>the red line (VmRss) shows the total amount of RAM that the OS allocates for PyPy: it includes both the memory directly handled by our GC and the \"raw memory\" that we need to allocate for other tasks, such as the assembly code generated by the JIT</li> <li>the brown line (gc-before) shows how much memory is used by the GC before each major collection</li> <li>the yellow line (gc-after) shows how much memory is used by the GC after each major collection: this represent the amount of memory which is actually needed to hold our Python objects.  The difference between gc-before and gc-after (the GC delta) is the amout of memory that the GC uses before triggering a new major collection</li> </ul> <p>By comparing gc-after and cpython-scaled, we can see that PyPy uses mostly the same amount of memory as CPython for storing the application objects (due to reference counting the memory usage in CPython is always very close to the actually necessary memory).  The extra memory used by PyPy is due to the GC delta, to the machine code generated by the JIT and probably to some other external effect (such as e.g. Memory Fragmentation).</p> <p>Note that the GC delta can be set arbitrarly low (another recent addition -- the default value depends on the actual RAM on your computer; it probably works to translate if your computer has precisely 2 GB, because in this case the GC delta and thus the total memory usage will be somewhat lower than reported here), but the cost is to have more frequent major collections and thus a higher run-time overhead.  The same is true for the memory needed by the JIT, which can be reduced by telling the JIT to compile less often or to discard old loops more frequently.  As often happens in computer science, there is a trade-off between space and time, and currently for this particular example PyPy runs twice as fast as CPython by doubling the memory usage. We hope to improve even more on this trade-off.</p> <p>On 64-bit, things are even better as shown by the the following graph:</p> <p>The general shape of the lines is similar to the 32-bit graph. However, the relative difference to CPython is much better: we need about 3 GB of RAM, just 24% more than the 2.4 GB needed by CPython.  And we are still more than 2x faster!</p> <p>The memory saving is due (partly?) to the vtable ptr optimization, which is enabled by default on 64-bit because it has no speed penalty (see Unifying the vtable ptr with the GC header).</p> <p>The net result of our work is that now translating PyPy on PyPy is practical and takes less than 30 minutes.  It's impressive how quickly you get used to translation taking half the time -- now we cannot use CPython any more for that because it feels too slow :-).</p>","tags":["pypy"]},{"location":"2010/11/09/a-snake-which-bites-its-tail-pypy-jitting-itself/","title":"A snake which bites its tail: PyPy JITting itself","text":"<p>Originally published on the PyPy blog.</p> <p>We have to admit: even if we have been writing for years about the fantastic speedups that the PyPy JIT gives, we, the PyPy developers, still don't use it for our daily routine.  Until today :-). </p> <p>Readers brave enough to run translate.py to translate PyPy by themselves surely know that the process takes quite a long time to complete, about a hour on super-fast hardware and even more on average computers.  Unfortunately, it happened that translate.py was a bad match for our JIT and thus ran much slower on PyPy than on CPython.</p> <p>One of the main reasons is that the PyPy translation toolchain makes heavy use of custom metaclasses, and until few weeks ago metaclasses disabled some of the central optimizations which make PyPy so fast.  During the recent D\u00fcsseldorf sprint, Armin and Carl Friedrich fixed this problem and re-enabled all the optimizations even in presence of metaclasses.</p> <p>So, today we decided that it was time to benchmark again PyPy against itself. First, we tried to translate PyPy using CPython as usual, with the following command line (on a machine with an \"Intel(R) Xeon(R) CPU W3580 @ 3.33GHz\" and 12 GB of RAM, running a 32-bit Ubuntu):</p> <pre>\n$ python ./translate.py -Ojit targetpypystandalone --no-allworkingmodules\n\n... lots of output, fractals included ...\n\n[Timer] Timings:\n[Timer] annotate                       ---  252.0 s\n[Timer] rtype_lltype                   ---  199.3 s\n[Timer] pyjitpl_lltype                 ---  565.2 s\n[Timer] backendopt_lltype              ---  217.4 s\n[Timer] stackcheckinsertion_lltype     ---   26.8 s\n[Timer] database_c                     ---  234.4 s\n[Timer] source_c                       ---  480.7 s\n[Timer] compile_c                      ---  258.4 s\n[Timer] ===========================================\n[Timer] Total:                         --- 2234.2 s\n</pre> <p>Then, we tried the same command line with PyPy (SVN revision 78903, x86-32 JIT backend, downloaded from the nightly build page):</p> <pre>\n$ pypy-c-78903 ./translate.py -Ojit targetpypystandalone --no-allworkingmodules\n\n... lots of output, fractals included ...\n\n[Timer] Timings:\n[Timer] annotate                       ---  165.3 s\n[Timer] rtype_lltype                   ---  121.9 s\n[Timer] pyjitpl_lltype                 ---  224.0 s\n[Timer] backendopt_lltype              ---   72.1 s\n[Timer] stackcheckinsertion_lltype     ---    7.0 s\n[Timer] database_c                     ---  104.4 s\n[Timer] source_c                       ---  167.9 s\n[Timer] compile_c                      ---  320.3 s\n[Timer] ===========================================\n[Timer] Total:                         --- 1182.8 s\n</pre> <p>Yes, it's not a typo: PyPy is almost two times faster than CPython! Moreover, we can see that PyPy is faster in each of the individual steps apart compile_c, which consists in just a call to make to invoke gcc. The slowdown comes from the fact that the Makefile also contains a lot of calls to the trackgcroot.py script, which happens to perform badly on PyPy but we did not investigate why yet.</p> <p>However, there is also a drawback: on this specific benchmark, PyPy consumes much more memory than CPython.  The reason why the command line above contains --no-allworkingmodules is that if we include all the modules the translation crashes when it's complete at 99% because it consumes all the 4GB of memory which is addressable by a 32-bit process.</p> <p>A partial explanation if that so far the assembler generated by the PyPy JIT is immortal, and the memory allocated for it is never reclaimed.  This is clearly bad for a program like translate.py which is divided into several independent steps, and for which most of the code generated in each step could be safely be thrown away when it's completed.</p> <p>If we switch to 64-bit we can address the whole 12 GB of RAM that we have, and thus translating with all working modules is no longer an issue.  This is the time taken with CPython (note that it does not make sense to compare with the 32-bit CPython translation above, because that one does not include all the modules):</p> <pre>\n$ python ./translate.py -Ojit\n\n[Timer] Timings:\n[Timer] annotate                       ---  782.7 s\n[Timer] rtype_lltype                   ---  445.2 s\n[Timer] pyjitpl_lltype                 ---  955.8 s\n[Timer] backendopt_lltype              ---  457.0 s\n[Timer] stackcheckinsertion_lltype     ---   63.0 s\n[Timer] database_c                     ---  505.0 s\n[Timer] source_c                       ---  939.4 s\n[Timer] compile_c                      ---  465.1 s\n[Timer] ===========================================\n[Timer] Total:                         --- 4613.2 s\n</pre> <p>And this is for PyPy:</p> <pre>\n$ pypy-c-78924-64 ./translate.py -Ojit\n\n[Timer] Timings:\n[Timer] annotate                       ---  505.8 s\n[Timer] rtype_lltype                   ---  279.4 s\n[Timer] pyjitpl_lltype                 ---  338.2 s\n[Timer] backendopt_lltype              ---  125.1 s\n[Timer] stackcheckinsertion_lltype     ---   21.7 s\n[Timer] database_c                     ---  187.9 s\n[Timer] source_c                       ---  298.8 s\n[Timer] compile_c                      ---  650.7 s\n[Timer] ===========================================\n[Timer] Total:                         --- 2407.6 s\n</pre> <p>The results are comparable with the 32-bit case: PyPy is still almost 2 times faster than CPython.  And it also shows that our 64-bit JIT backend is as good as the 32-bit one.  Again, the drawback is in the consumed memory: CPython used 2.3 GB while PyPy took 8.3 GB.</p> <p>Overall, the results are impressive: we knew that PyPy can be good at optimizing small benchmarks and even middle-sized programs, but as far as we know this is the first example in which it heavily optimizes a huge, real world application.  And, believe us, the PyPy translation toolchain is complex enough to contains all kinds of dirty tricks and black magic that make Python lovable and hard to optimize :-).</p>","tags":["pypy"]},{"location":"2010/12/14/pypy-migrates-to-mercurial/","title":"PyPy migrates to Mercurial","text":"<p>Originally published on the PyPy blog.</p> <p>The assiduous readers of this blog surely remember that during the last D\u00fcsseldorf sprint in October, we started the process for migrating our main development repository from Subversion to Mercurial.  Today, after more than two months, the process has finally been completed :-). </p> <p>The new official PyPy repository is hosted on BitBucket.</p> <p>The migration has been painful because the SVN history of PyPy was a mess and none of the existing conversion tools could handle it correctly.  This was partly because PyPy started when subversion was still at version 0.9 when some best-practices were still to be established, and partly because we probably managed to invent all the possible ways to do branches (and even some of the impossible ones: there is at least one commit which you cannot do with the plain SVN client but you have to speak to the server by yourself :-)).</p> <p>The actual conversion was possible thanks to the enormous work done by Ronny Pfannschmidt and his hackbeil tool. I would like to personally thank Ronny for his patience to handle all the various requests we asked for.</p> <p>We hope that PyPy development becomes even more approachable now, at least from a version control point of view.</p>","tags":["pypy"]},{"location":"2011/01/21/pypy-wants-you/","title":"PyPy wants you!","text":"<p>Originally published on the PyPy blog.</p> <p>If you ever considered contributing to PyPy, but never did so far, this is a good moment to start! :-) </p> <p>Recently, we merged the fast-forward branch which brings Python 2.7 compatibility, with the plan of releasing a new version of PyPy as soon as all tests pass.</p> <p>However, at the moment there are still quite a few of failing tests because of new 2.7 features that have not been implemented yet: many of them are easy to fix, and doing it represents a good way to get confidence with the code base, for those who are interested in it. Michael Foord wrote a little howto explaining the workflow for running lib-python tests.</p> <p>Thus, if you are willing to join us in the effort of having a PyPy compatible with Python 2.7, probably the most sensible option is to come on the #PyPy IRC channel on Freenode, so we can coordinate each other not to fix the same test twice.</p> <p>Moreover, if you are a student and are considering participating in the next Google Summer of Code this is a good time to get into pypy. You have the opportunity to get a good understanding of pypy for when you decide what you would like to work on over the summer.</p>","tags":["pypy"]},{"location":"2011/04/20/using-tkinter-and-idle-with-pypy/","title":"Using Tkinter and IDLE with PyPy","text":"<p>Originally published on the PyPy blog.</p> <p>We are pleased to announce that Tkinter, the GUI library based on TCL/TK, now works with PyPy. Tkinter is composed of two parts: </p> <ul> <li>_tkinter, a module written in C which interfaces with the TCL world</li> <li>Tkinter, a pure Python package which wraps _tkinter to expose the pythonic API we are used to</li> </ul>  The PyPy version of _tkinter reuses the C code of as found in CPython and compile it through the PyPy C-API compatibility layer, cpyext.  To make it work with PyPy, we had to modify it slightly, in order to remove the dependency on some API functions which are not supported by PyPy.  In particular, we removed the dependency on the PyOS_InputHook variable, which allows a nice integration of Tkinter and the Python interactive prompt: the result is that, unlike CPython, in PyPy Tk windows created at the interactive prompt are not shown until we manually call the mainloop method.  Apart from this inconvenience, all the rest works fine. At the moment, _tkinter is not distributed with PyPy because our build system does not support automatic compilation of C extension.  Instead, it is necessary to install it manually, either directly from source or by easy_installing/pip installing tkinter-pypy from PyPI. For everything to work correctly, you need a recent build of PyPy: the following is a step-by-step guide to install _tkinter in a PyPy nightly build for Linux 64 bit; for other architectures, look at the nightly build page: <pre>$ wget https://buildbot.pypy.org/nightly/trunk/pypy-c-jit-43485-1615dfd7d8f1-linux64.tar.bz2\n\n$ tar xfv pypy-c-jit-43485-1615dfd7d8f1-linux64.tar.bz2\n\n$ cd pypy-c-jit-43485-1615dfd7d8f1-linux64/\n\n$ wget https://peak.telecommunity.com/dist/ez_setup.py\n\n$ ./bin/pypy ez_setup.py    # install setuptools\n\n$ ./bin/easy_install tkinter-pypy\n</pre> Once you complete the steps above, you can start using Tkinter from your python programs.  In particular, you can use IDLE, the IDE which is part of the Python standard library.  To start IDLE, type: <pre>$ ./bin/pypy -m idlelib.idle\n</pre> Have fun :-)","tags":["pypy"]},{"location":"2011/05/23/pypy-genova-pegli-post-europython-sprint-june-27---july-2-2011/","title":"PyPy Genova-Pegli Post-EuroPython Sprint June 27 - July 2 2011","text":"<p>Originally published on the PyPy blog.</p> <p>The next PyPy sprint will be in Genova-Pegli, Italy, the week after EuroPython (which is in Florence, about 3h away by train). This is a fully public sprint: newcomers and topics other than those proposed below are welcome. </p>   Goals and topics of the sprint <ul> <li> Now that we have released 1.5, the sprint itself is going to be mainly working on fixing issues reported by various users.  Possible topics include, but are not limited to: <ul> <li>fixing issues in the bug tracker</li> <li>improve cpyext, the C-API compatibility layer, to support more extension modules</li> <li>finish/improve/merge jitypes2, the branch which makes ctypes JIT friendly</li> <li>general JIT improvements</li> <li>improve our tools, like the jitviewer or the buildbot infrastructure</li> <li>make your favorite module/application working on PyPy, if it doesn't yet</li> </ul> </li> <li> Of course this does not prevent people from showing up with a more precise interest in mind  If there are newcomers, we will gladly give introduction talks. </li> <li> Since we are almost on the beach, we can take one day off for summer relaxation and/or tourist visits nearby :-). </li> </ul>   Exact times The work days should be 27 June - 2 July 2011.  People may arrive on the 26th already and/or leave on the 3rd.   Location &amp; Accomodation Both the sprint venue and the lodging will be at Albergo Puppo in Genova-Pegli, Italy.  Pegli is a nice and peaceful little quarter of Genova, and the hotel is directly on the beach, making it a perfect place for those who want to enjoy the sea in the middle of the Italian summer, as a quick search on Google Images shows :-)  The place has a good ADSL Internet connexion with wireless installed.  You can of course arrange your own lodging anywhere but I definitely recommend lodging there too. Please confirm that you are coming so that we can adjust the reservations as appropriate.  The prices are as follows, and they include breakfast and a parking place for the car, in case you need it: <ul> <li>single room:  70 \u20ac</li> <li>double room:  95 \u20ac</li> <li>triple room: 105 \u20ac</li> </ul>  Please register by hg: https://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/genova-pegli-2011/people.txt or on the pypy-dev mailing list if you do not yet have check-in rights: https://mail.python.org/mailman/listinfo/pypy-dev In case you want to share a room with someone else but you don't know who, please let us know (either by writing it directly in people.txt or by writing on the mailing list) and we will try to arrange it.","tags":["pypy"]},{"location":"2011/07/07/realtime-image-processing-in-python/","title":"Realtime image processing in Python","text":"<p>Originally published on the PyPy blog.</p> <p>Image processing is notoriously a CPU intensive task.  To do it in realtime, you need to implement your algorithm in a fast language, hence trying to do it in Python is foolish: Python is clearly not fast enough for this task. Is it? :-) Actually, it turns out that the PyPy JIT compiler produces code which is fast enough to do realtime video processing using two simple algorithms implemented by H\u00e5kan Ard\u00f6. sobel.py implements a classical way of locating edges in images, the Sobel operator. It is an approximation of the magnitude of the image gradient. The processing time is spend on two convolutions between the image and 3x3-kernels. magnify.py implements a pixel coordinate transformation that rearranges the pixels in the image to form a magnifying effect in the center. It consists of a single loop over the pixels in the output image copying pixels from the input image. You can try by yourself by downloading the appropriate demo: </p> <ul> <li>pypy-image-demo.tar.bz2: this archive contains only the source code, use this is you have PyPy already installed</li> <li>pypy-image-demo-full.tar.bz2: this archive contains both the source code and prebuilt PyPy binaries for linux 32 and 64 bits</li> </ul>  To run the demo, you need to have mplayer installed on your system.  The demo has been tested only on linux, it might (or not) work also on other systems: <pre>$ pypy pypy-image-demo/sobel.py\n\n$ pypy pypy-image-demo/magnify.py\n</pre> By default, the two demos uses an example AVI file.  To have more fun, you can use your webcam by passing the appropriate mplayer parameters to the scripts, e.g: <pre>$ pypy demo/sobel.py tv://\n</pre> By default magnify.py uses nearest-neighbor interpolation.  By adding the option -b, bilinear interpolation will be used instead, which gives smoother result: <pre>$ pypy demo/magnify.py -b\n</pre> There is only a single implementation of the algorithm in magnify.py. The two different interpolation methods are implemented by subclassing the class used to represent images and embed the interpolation within the pixel access method. PyPy is able to achieve good performance with this kind of abstractions because it can inline the pixel access method and specialize the implementation of the algorithm. In C++ that kind of pixel access method would be virtual and you'll need to use templates to get the same effect without incurring in runtime overhead.  The video above shows PyPy and CPython running sobel.py side by side (PyPy taking input from the webcam, CPython from the test file). Alternatively, to have a feeling on how much PyPy is faster than CPython, try to run the demo with the latter.  These are the the average fps (frames per second) that I get on my machine (Ubuntu 64 bit, Intel i7 920, 4GB RAM) when processing the default test.avi video and using the prebuilt PyPy binary found in the full tarball alinked above.  For sobel.py: <ul> <li>PyPy: ~47.23 fps</li> <li>CPython: ~0.08 fps</li> </ul>  For magnify.py: <ul> <li>PyPy: ~26.92 fps</li> <li>CPython: ~1.78 fps</li> </ul>  This means that on sobel.py, PyPy is 590 times faster.  On magnify.py the difference is much less evident and the speedup is \"only\" 15x. It must be noted that this is an extreme example of what PyPy can do.  In particular, you cannot expect (yet :-)) PyPy to be fast enough to run an arbitrary video processing algorithm in real time, but the demo still proves that PyPy has the potential to get there.","tags":["pypy"]},{"location":"2011/11/14/gothenburg-sprint-report/","title":"Gothenburg sprint report","text":"<p>Originally published on the PyPy blog.</p> <p>In the past week, we have been busy hacking on PyPy at the Gothenburg sprint, the second of this 2011.  The sprint was hold at Laura's and Jacob's place, and here is a brief report of what happened.  In the first day we welcomed Mark Pearse, who was new to PyPy and at his first sprint.  Mark worked the whole sprint in the new SpecialisedTuple branch, whose aim is to have a special implementation for small 2-items and 3-items tuples of primitive types (e.g., ints or floats) to save memory.  Mark paired with Antonio for a couple of days, then he continued alone and did an amazing job.  He even learned how to properly do Test Driven Development :-). Antonio spent a couple of days investigating whether it is possible to use application checkpoint libraries such as BLCR and DMTCP to save the state of the PyPy interpreter between subsequent runs, thus saving also the JIT-compiled code to reduce the warmup time.  The conclusion is that these are interesting technologies, but more work would be needed (either on the PyPy side or on the checkpoint library side) before it can have a practical usage for PyPy users. Then, Antonio spent most of the rest of the sprint working on his ffistruct branch, whose aim is to provide a very JIT-friendly way to interact with C structures, and eventually implement ctypes.Structure on top of that.  The \"cool part\" of the branch is already done, and the JIT already can compile set/get of fields into a single fast assembly instruction, about 400 times faster than the corresponding ctypes code.  What is still left to do is to add a nicer syntax (which is easy) and to implement all the ctypes peculiarities (which is tedious, at best :-)). As usual, Armin did tons of different stuff, including fixing a JIT bug, improving the performance of file.readlines() and working on the STM branch (for Software Transactional Memory), which is now able to run RPython multithreaded programs using software transaction (as long as they don't fill up all the memory, because support for the GC is still missing :-)).  Finally, he worked on improving the Windows version of PyPy. While doing so he discovered together with Anto a terrible bug which lead to a continuous leak of stack space because the JIT called some functions using the wrong calling convention. H\u00e5kan, with some help from Armin, worked on the jit-targets branch, whose goal is to heavily refactor the way the traces are internally represented by the JIT, so that in the end we can produce (even :-)) better code than what we do nowadays.  More details in this mail. Andrew Dalke worked on a way to integrate PyPy with FORTRAN libraries, and in particular the ones which are wrapped by Numpy and Scipy: in doing so, he wrote f2pypy, which is similar to the existing f2py but instead of producing a CPython extension module it produces a pure python modules based on ctypes.  More work is needed before it can be considered complete, but f2pypy is already able to produce a wrapper for BLAS which passes most of the tests under CPython, although there's still work left to get it working for PyPy. </p> Armin and H\u00e5kan with Laura's \"5x faster\" cake Christian Tismer worked the whole sprint on the branch to make PyPy compatible with Windows 64 bit.  This needs a lot of work because a lot of PyPy is written under the assumption that the long type in C has the same bit size than void*, which is not true on Win64.  Christian says that in the past Genova-Pegli sprint he completed 90% of the work, and in this sprint he did the other 90% of the work.  Obviously, what is left to complete the task is the third 90% :-).  More seriously, he estimated a total of 2-4 person-weeks of work to finish it. But, all in all, the best part of the sprint has been the cake that Laura baked to celebrate the \"5x faster than CPython\" achievement. Well, actually our speed page reports \"only\" 4.7x, but that's because in the meantime we switched from comparing against CPython 2.6 to comparing against CPython 2.7, which is slightly faster.  We are confident that we will reach the 5x goal again, and that will be the perfect excuse to eat another cake :-)","tags":["pypy"]},{"location":"2012/02/16/py3k-status-update/","title":"Py3k status update","text":"<p>Originally published on the PyPy blog.</p> Thank to all the people who donated to the py3k proposal, we managed to collect enough money to start to work on the first step.  This is a quick summary of what I did since I began working on this.    First of all, many thanks to Amaury Forgeot d'Arc, who started the py3k branch months ago, and already implemented lots of features including e.g. switching to \"unicode everywhere\" and the int/long unification, making my job considerably easier :-) I started to work on the branch at the last Leysin sprint together with Romain Guillebert, where we worked on various syntactical changes such as extended tuple unpacking and keyword-only arguments.  Working on such features is a good way to learn about a lot of the layers which the PyPy Python interpreter is composed of, because often you have to touch the tokenizer, the parser, the ast builder, the compiler and finally the interpreter. Then I worked on improving our test machinery in various way, e.g. by optimizing the initialization phase of the object space created by tests, which considerably speeds up small test runs, and adding the possibility to automatically run our tests against CPython 3, to ensure that what we are not trying to fix a test which is meant to fail :-). I also setup our buildbot to run the py3k tests nightly, so that we can have an up to date overview of what is left to do. Finally I started to look at all the tests in the interpreter/ directory, trying to unmangle the mess of failing tests. Lots of tests were failing because of simple syntax errors (e.g., by using the no longer valid except Exception, e syntax or the old print statement), others for slightly more complex reasons like unicode vs bytes or the now gone int/long distinction.  Others were failing simply because they relied on new features, such as the new lexical exception handlers. To give some numbers, at some point in january we had 1621 failing tests in the branch, while today we are under 1000 (to be exact: 999, and this is why I've waited until today to post the status update :-)). Before ending this blog post, I would like to thank once again all the people who donated to PyPy, who let me to do this wonderful job.  That's all for now, I'll post more updates soon. cheers, Antonio","tags":["pypy3","pypy"]},{"location":"2012/03/01/py3k-status-update-2/","title":"Py3k status update #2","text":"<p>Originally published on the PyPy blog.</p> <p>This is the second status update about my work on the py3k branch, which I can work on thanks to all of the people who donated to the py3k proposal. </p> <p>Since my previous status update, things have improved a lot: first of all, I fixed the syntax of many more tests, which were failing on the branch because they used constructs which are no longer valid in Python 3, such as u'' strings, the print statement or the old except Exception, e syntax.  I have to say that this work is tedious and not very rewarding, but it has to be done anyway, so that the real failures can stand up.</p><p>Then, I spent most of the rest of the time by killing features which are present in Python 2 and are gone in Python 3.</p><p>Some of them were easy and mechnical: for example, I removed all the function attributes such as func_code and func_closure, which has been renamed to __code__ and __closure__, and then I had to find and fix all the places which still expected the old ones.</p><p>Some were trickier: I removed support for the cmp function and the __cmp__ special method, but this also meant that I had to fix a few types which relied on it to be comparable (for example, did you know that the cells contained in __closure__ are comparable?). At the same time, I also removed the old behavior which in Python 2 allows us to compare arbitrary objects with &lt;, &gt; &amp; co.: in Python 3 the only comparisons allowed between incompatible types are == and !=.</p><p>Speaking of old special methods, __hex__ and __oct__ are gone as well (and I didn't even know about their existence before removing them :-))</p><p>But the most important breakthrough was the removal of the _file module, containing the implementation of the file type in Python 2, which is now gone since in Python 3 files are handled by the _io module.  Killing the module was not straightforward, because some of the importing logic was tightly tied to the internal implementation of files, so it needed some refactoring. Finally, I had to fix the marshal module to correctly detect text files vs. byte files.</p><p>Among these things, I fixed tons of smaller issues here and there. As a result, there are many fewer failing tests than a few weeks ago.  Obviously the number itself does not mean much, because sometimes fixing a single test takes hours, and some other times by changing one line one fixes tens of tests. But at the end, seeing it dropping from 999 to 650 always is nice and rewarding :-).</p><p>The road for having a pypy3k is still long, but everything is going fine so far. Stay tuned for more updates!</p><p>cheers, Antonio</p>","tags":["pypy3","pypy"]},{"location":"2012/04/06/py3k-status-update-3/","title":"Py3k status update #3","text":"<p>Originally published on the PyPy blog.</p> This is the third status update about my work on the py3k branch, which I can work on thanks to all of the people who donated to the py3k proposal.   A lot of work has been done during the last month: as usual, the list of changes is too big to be reported in a detalied way, so this is just a summary of what happened.  One of the most active areas was killing old and deprecated features. In particular, we killed support for the __cmp__ special method and its counsins, the cmp builtin function and keyword argument for list.sort() and sorted().  Killing is easy, but then you have to fix all the places which breaks because of this, including all the types which relied on __cmp__ to be comparable,, fixing all the tests which tried to order objects which are no longer ordeable now, or implementing new behavior like forbidding calling hash() on objects which implement __eq__ but not __hash__.  Among the other features, we killed lots of now-gone functions in the operator module, the builtins apply(), reduce() and buffer, and the os.* functions to deal with temporary files, which has been deprecated in favour of the new tempfile module.  The other topic which can't miss in a py3k status update is, as usual, string-vs-unicode. At this round, we fixed bugs in string formatting (in particular to teach format() to always use unicode strings) and various corner cases about when calling the (possibly overridden) __str__ method on subclasses of str. Believe me, you don't want to know the precise rules :-).  Other features which we worked on and fixed tests include, but are not limited to, marshal, hashlib, zipimport, _socket and itertools, plus the habitual endless lists of tests which fail for shallow reasons such as the syntactic differences, int vs long, range() vs list(range()) etc. As a result, the number of failing tests dropped from 650 to 235: we are beginning to see the light at the end of the tunnel :-)  Benjamin finished implementing Python 3 syntax. Most of it was small cleanups and tweaks to be compatible with CPython such as making True and False keywords and preventing . . . (note spaces between dots) from being parsed as Ellipsis. Larger syntax additions included keyword only arguments and function annotations.  Finally, we did some RPython fixes, so that it is possible again to translate PyPy in the py3k branch. However, the resuling binary is a strange beast which mixes python 2 and python 3 semantics, so it is unusable for anything but showing friends how cool it is.  I would like to underline that I was not alone in doing all this work. In particular, a lot of people joined the PyPy sprint at Pycon and worked on the branch, as you can clearly see in this activity graph. I would like to thank all who helped!  cheers, Antonio and Benjamin","tags":["pypy3","pypy"]},{"location":"2012/06/05/py3k-status-update-4/","title":"Py3k status update #4","text":"<p>Originally published on the PyPy blog.</p> <p>This is the fourth status update about our work on the py3k branch, which we can work on thanks to all of the people who donated to the py3k proposal. </p> <p>For various reasons, less work than usual has been done since the last status update. However, some interesting things happened anyway.</p><p>As readers know, so far we spent most of the effort in fixing all PyPy's own tests which started to fail for various py2/py3 differences.  Most of them failed for shallow reasons, e.g. syntactic changes or the int/long unifications. Others failed for subtle differences and needed a bit more care, for example the fact that unbound methods are gone in Py3k.</p><p>The good news is that finally we are seeing the light at the end of the tunnel. Most of them have been fixed. For sine other tests, we introduced the concept of \"py3k-skipping\": some optimizations and modules are indeed failing, but right now we are concentrating on completing the core language and so we are not interested in those.  When the core language will be done, we will be able to easily find and work on the py3k-skipped tests.  In particular, for now we disabled the Int and String dict strategies, which are broken because of the usual int/long unification and str vs bytes.  As for modules, for now _continuation (needed for stackless) and _multiprocessing do not work yet.</p><p>Another non-trivial feature we implemented is the proper cleaning of exception variables when we exit except blocks.  This is a feature which touches lots of levels of PyPy, starting from astcompiler, down to the bytecode interpreter. It tooks two days of headache, but at the end we made it :-).</p><p>Additionally, Amaury did a lot of improvements to cpyext, which had been broken since forever on this branch.</p><p>As for the next plans, now that things are starting to work and PyPy's own tests mostly pass, we can finally start to run the compiled PyPy against CPython's test suite.  It is very likely that we will have tons of failures at the beginning, but once we start to fix them one by one, a Py3k-compatible PyPy will be closer and closer.</p>","tags":["pypy3","pypy"]},{"location":"2012/07/10/py3k-status-update-5/","title":"Py3k status update #5","text":"<p>Originally published on the PyPy blog.</p> <p>This is the fifth status update about our work on the py3k branch, which we can work on thanks to all of the people who donated to the py3k proposal. </p> <p>Apart from the usual \"fix shallow py3k-related bugs\" part, most of my work in this iteration has been to fix the bootstrap logic of the interpreter, in particular to setup the initial sys.path.</p><p>Until few weeks ago, the logic to determine sys.path was written entirely at app-level in pypy/translator/goal/app_main.py, which is automatically included inside the executable during translation.  The algorithm is more or less like this:</p><ol><li>find the absolute path of the executable by looking at sys.argv[0] and cycling through all the directories in PATH</li> <li>starting from there, go up in the directory hierarchy until we find a directory which contains lib-python and lib_pypy</li> </ol><p>This works fine for Python 2 where the paths and filenames are represented as 8-bit strings, but it is a problem for Python 3 where we want to use unicode instead.  In particular, whenever we try to encode a 8-bit string into an unicode, PyPy asks the _codecs built-in module to find the suitable codec. Then, _codecs tries to import the encodings package, to list all the available encodings. encodings is a package of the standard library written in pure Python, so it is located inside lib-python/3.2. But at this point in time we yet have to add lib-python/3.2 to sys.path, so the import fails.  Bootstrap problem!</p><p>The hard part was to find the problem: since it is an error which happens so early, the interpreter is not even able to display a traceback, because it cannot yet import traceback.py. The only way to debug it was through some carefully placed print statement and the help of gdb. Once found the problem, the solution was as easy as moving part of the logic to RPython, where we don't have bootstrap problems.</p><p>Once the problem was fixed, I was able to finally run all the CPython test against the compiled PyPy.  As expected there are lots of failures, and fixing them will be the topic of my next months.</p>","tags":["pypy3","pypy"]},{"location":"2012/09/26/py3k-status-update-6/","title":"Py3k status update #6","text":"<p>Originally published on the PyPy blog.</p> <p>This is the sixth status update about our work on the py3k branch, which we can work on thanks to all of the people who donated to the py3k proposal. </p> <p>The coolest news is not about what we did in the past weeks, but what we will do in the next: I am pleased to announce that Philip Jenvey has been selected by the PyPy communitiy to be funded for his upcoming work on py3k, thanks to your generous donations. He will start to work on it shortly, and he will surely help the branch to make faster progress.  I am also particularly happy of this because Philip is the first non-core developer who is getting paid with donations: he demonstrated over the past months to be able to work effectively on PyPy, and so we were happy to approve his application for the job.  This means that anyone can potentially be selected in the future, the only strict requirement is to have a deep interest in working on PyPy and to prove to be able to do so by contributing to the project.</p><p>Back to the status of the branch. Most of the work since the last status update has been done in the area of, guess what? Unicode strings. As usual, this is one of the most important changes between Python 2 and Python 3, so it's not surprising.  The biggest news is that now PyPy internally supports unicode identifiers (such as names of variables, functions, attributes, etc.), whereas earlier it supported only ASCII bytes strings.  The changes is still barely visible from the outside, because the parser still rejects non-ASCII identifiers, however you can see it with a bit of creativity:</p><pre>&gt;&gt;&gt;&gt; def foo(x): pass\n&gt;&gt;&gt;&gt; foo(**{'\u00e0\u00e8\u00ec\u00f2\u00f9': 42})\nTraceback (most recent call last):\n  File \"&lt;console&gt;\", line 1, in &lt;module&gt;\nTypeError: foo() got an unexpected keyword argument '\u00e0\u00e8\u00ec\u00f2\u00f9'\n</pre><p>Before the latest changes, you used to get question marks instead of the proper name for the keyword argument.  Although this might seem like a small detail, it is a big step towards a proper working Python 3 interpreter and it required a couple of days of headaches.  A spin-off of this work is that now RPython has better built-in support for unicode (also in the default branch): for example, it now supports unicode string formatting (using the percent operator) and the methods .encode/.decode('utf-8').</p><p>Other than that there is the usual list of smaller issues and bugs that got fixed, including (but not limited to):</p><ul><li>teach the compiler when to emit the new opcode DELETE_DEREF (and implement it!)</li> <li>detect when we use spaces and TABs inconsistently in the source code, as CPython does</li> <li>fix yet another bug related to the new lexically scoped exceptions (this is the last one, hopefully)</li> <li>port some of the changes that we did to the standard CPython 2.7 tests to 3.2, to mark those which are implementation details and should not be run on PyPy</li> </ul><p>Finally, I would like to thank Amaury Forgeot d'Arc and Ariel Ben-Yehuda for their work on the branch; among other things, Amaury recently worked on cpyext and on the PyPy _cffi_backend, while Ariel submitted a patch to implement PEP 3138.</p>","tags":["pypy3","pypy"]},{"location":"2017/07/26/binary-wheels-for-pypy/","title":"Binary wheels for PyPy","text":"<p>Originally published on the PyPy blog.</p> <p>Hi,  this is a short blog post, just to announce the existence of this Github repository, which contains binary PyPy wheels for some selected packages. The availability of binary wheels means that you can install the packages much more quickly, without having to wait for compilation. </p>  At the moment of writing, these packages are available: <ul> <li>numpy</li> <li>scipy</li> <li>pandas</li> <li>psutil</li> <li>netifaces</li> </ul>  For now, we provide only wheels built on Ubuntu, compiled for PyPy 5.8. In particular, it is worth noting that they are not manylinux1 wheels, which means they could not work on other Linux distributions. For more information, see the explanation in the README of the above repo.  Moreover, the existence of the wheels does not guarantee that they work correctly 100% of the time. they still depend on cpyext, our C-API emulation layer, which is still work-in-progress, although it has become better and better during the last months. Again, the wheels are there only to save compilation time.  To install a package from the wheel repository, you can invoke pip like this: $ pip install --extra-index https://antocuni.github.io/pypy-wheels/ubuntu numpy  Happy installing!","tags":["pypy"]},{"location":"2017/10/18/cape-of-good-hope-for-pypy/","title":"(Cape of) Good Hope for PyPy","text":"<p>Originally published on the PyPy blog.</p>  Hello from the other side of the world (for most of you)!  With the excuse of coming to PyCon ZA during the last two weeks Armin, Ronan, Antonio and sometimes Maciek had a very nice and productive sprint in Cape Town, as pictures show :). We would like to say a big thank you to Kiwi.com, which sponsored part of the travel costs via its awesome Sourcelift program to help Open Source projects. Armin, Anto and Ronan at Cape Point  Armin, Ronan and Anto spent most of the time hacking at cpyext, our CPython C-API compatibility layer: during the last years, the focus was to make it working and compatible with CPython, in order to run existing libraries such as numpy and pandas. However, we never paid too much attention to performance, so the net result is that with the latest released version of PyPy, C extensions generally work but their speed ranges from \"slow\" to \"horribly slow\".  For example, these very simple microbenchmarks measure the speed of calling (empty) C functions, i.e. the time you spend to \"cross the border\" between RPython and C.  (Note: this includes the time spent doing the loop in regular Python code.) These are the results on CPython, on PyPy 5.8, and on our newest in-progress version: <pre>$ python bench.py     # CPython\nnoargs      : 0.41 secs\nonearg(None): 0.44 secs\nonearg(i)   : 0.44 secs\nvarargs     : 0.58 secs\n</pre> <pre>$ pypy-5.8 bench.py   # PyPy 5.8\nnoargs      : 1.01 secs\nonearg(None): 1.31 secs\nonearg(i)   : 2.57 secs\nvarargs     : 2.79 secs\n</pre> <pre>$ pypy bench.py       # cpyext-refactor-methodobject branch\nnoargs      : 0.17 secs\nonearg(None): 0.21 secs\nonearg(i)   : 0.22 secs\nvarargs     : 0.47 secs\n</pre> <pre></pre> <pre></pre> So yes: before the sprint, we were ~2-6x slower than CPython. Now, we are faster than it! To reach this result, we did various improvements, such as:  <ol> <li>teach the JIT how to look (a bit) inside the cpyext module;</li> <li>write specialized code for calling METH_NOARGS, METH_O and METH_VARARGS functions; previously, we always used a very general and slow logic;</li> <li>implement freelists to allocate the cpyext versions of int and tuple objects, as CPython does;</li> <li>the cpyext-avoid-roundtrip branch: crossing the RPython/C border is slowish, but the real problem was (and still is for many cases) we often cross it many times for no good reason. So, depending on the actual API call, you might end up in the C land, which calls back into the RPython land, which goes to C, etc. etc. (ad libitum).</li> </ol>  The branch tries to fix such nonsense: so far, we fixed only some cases, which are enough to speed up the benchmarks shown above.  But most importantly, we now have a clear path and an actual plan to improve cpyext more and more. Ideally, we would like to reach a point in which cpyext-intensive programs run at worst at the same speed of CPython.  The other big topic of the sprint was Armin and Maciej doing a lot of work on the unicode-utf8 branch: the goal of the branch is to always use UTF-8 as the internal representation of unicode strings. The advantages are various:  <ul> <li>decoding a UTF-8 stream is super fast, as you just need to check that the stream is valid;</li> <li>encoding to UTF-8 is almost a no-op;</li> <li>UTF-8 is always more compact representation than the currently used UCS-4. It's also almost always more compact than CPython 3.5 latin1/UCS2/UCS4 combo;</li> <li>smaller representation means everything becomes quite a bit faster due to lower cache pressure.</li> </ul>  Before you ask: yes, this branch contains special logic to ensure that random access of single unicode chars is still O(1), as it is on both CPython and the current PyPy. We also plan to improve the speed of decoding even more by using modern processor features, like SSE and AVX. Preliminary results show that decoding can be done 100x faster than the current setup.   In summary, this was a long and profitable sprint, in which we achieved lots of interesting results. However, what we liked even more was the privilege of doing commits from awesome places such as the top of Table Mountain:  Our sprint venue today #pypy pic.twitter.com/o38IfTYmAV \u2014 Ronan Lamy (@ronanlamy) 4 ottobre 2017 The panorama we looked at instead of staring at cpyext code","tags":["sprint","unicode","cpyext","profiling","speed","pypy"]},{"location":"2017/10/30/how-to-make-your-code-80-times-faster/","title":"How to make your code 80 times faster","text":"<p>Originally published on the PyPy blog.</p>  I often hear people who are happy because PyPy makes their code 2 times faster or so. Here is a short personal story which shows PyPy can go well beyond that.  DISCLAIMER: this is not a silver bullet or a general recipe: it worked in this particular case, it might not work so well in other cases. But I think it is still an interesting technique. Moreover, the various steps and implementations are showed in the same order as I tried them during the development, so it is a real-life example of how to proceed when optimizing for PyPy.  Some months ago I played a bit with evolutionary algorithms: the ambitious plan was to automatically evolve a logic which could control a (simulated) quadcopter, i.e. a PID controller (spoiler: it doesn't fly).  The idea is to have an initial population of random creatures: at each generation, the ones with the best fitness survive and reproduce with small, random variations.  However, for the scope of this post, the actual task at hand is not so important, so let's jump straight to the code. To drive the quadcopter, a Creature has a run_step method which runs at each delta_t (full code): <pre>class Creature(object):\n    INPUTS = 2  # z_setpoint, current z position\n    OUTPUTS = 1 # PWM for all 4 motors\n    STATE_VARS = 1\n    ...\n\n    def run_step(self, inputs):\n        # state: [state_vars ... inputs]\n        # out_values: [state_vars, ... outputs]\n        self.state[self.STATE_VARS:] = inputs\n        out_values = np.dot(self.matrix, self.state) + self.constant\n        self.state[:self.STATE_VARS] = out_values[:self.STATE_VARS]\n        outputs = out_values[self.STATE_VARS:]\n        return outputs\n</pre> <ul> <li>inputs is a numpy array containing the desired setpoint and the current position on the Z axis;</li> <li>outputs is a numpy array containing the thrust to give to the motors. To start easy, all the 4 motors are constrained to have the same thrust, so that the quadcopter only travels up and down the Z axis;</li> <li>self.state contains arbitrary values of unknown size which are passed from one step to the next;</li> <li>self.matrix and self.constant contains the actual logic. By putting the \"right\" values there, in theory we could get a perfectly tuned PID controller. These are randomly mutated between generations.</li> </ul> run_step is called at 100Hz (in the virtual time frame of the simulation). At each generation, we test 500 creatures for a total of 12 virtual seconds each. So, we have a total of 600,000 executions of run_step at each generation.  At first, I simply tried to run this code on CPython; here is the result: <pre>$ python -m ev.main\nGeneration   1: ... [population = 500]  [12.06 secs]\nGeneration   2: ... [population = 500]  [6.13 secs]\nGeneration   3: ... [population = 500]  [6.11 secs]\nGeneration   4: ... [population = 500]  [6.09 secs]\nGeneration   5: ... [population = 500]  [6.18 secs]\nGeneration   6: ... [population = 500]  [6.26 secs]\n</pre> Which means ~6.15 seconds/generation, excluding the first.  Then I tried with PyPy 5.9: <pre>$ pypy -m ev.main\nGeneration   1: ... [population = 500]  [63.90 secs]\nGeneration   2: ... [population = 500]  [33.92 secs]\nGeneration   3: ... [population = 500]  [34.21 secs]\nGeneration   4: ... [population = 500]  [33.75 secs]\n</pre> Ouch! We are ~5.5x slower than CPython. This was kind of expected: numpy is based on cpyext, which is infamously slow.  (Actually, we are working on that and on the cpyext-avoid-roundtrip branch we are already faster than CPython, but this will be the subject of another blog post.)  So, let's try to avoid cpyext. The first obvious step is to use numpypy instead of numpy (actually, there is a hack to use just the micronumpy part). Let's see if the speed improves: <pre>$ pypy -m ev.main   # using numpypy\nGeneration   1: ... [population = 500]  [5.60 secs]\nGeneration   2: ... [population = 500]  [2.90 secs]\nGeneration   3: ... [population = 500]  [2.78 secs]\nGeneration   4: ... [population = 500]  [2.69 secs]\nGeneration   5: ... [population = 500]  [2.72 secs]\nGeneration   6: ... [population = 500]  [2.73 secs]\n</pre> So, ~2.7 seconds on average: this is 12x faster than PyPy+numpy, and more than 2x faster than the original CPython. At this point, most people would be happy and go tweeting how PyPy is great.  In general, when talking of CPython vs PyPy, I am rarely satified of a 2x speedup: I know that PyPy can do much better than this, especially if you write code which is specifically optimized for the JIT. For a real-life example, have a look at capnpy benchmarks, in which the PyPy version is ~15x faster than the heavily optimized CPython+Cython version (both have been written by me, and I tried hard to write the fastest code for both implementations).  So, let's try to do better. As usual, the first thing to do is to profile and see where we spend most of the time. Here is the vmprof profile. We spend a lot of time inside the internals of numpypy, and allocating tons of temporary arrays to store the results of the various operations.  Also, let's look at the jit traces and search for the function run: this is loop in which we spend most of the time, and it is composed of 1796 operations.  The operations emitted for the line np.dot(...) + self.constant are listed between lines 1217 and 1456. Here is the excerpt which calls np.dot(...); most of the ops are cheap, but at line 1232 we see a call to the RPython function descr_dot; by looking at the implementation we see that it creates a new W_NDimArray to store the result, which means it has to do a malloc():  The implementation of the + self.constant part is also interesting: contrary the former, the call to W_NDimArray.descr_add has been inlined by the JIT, so we have a better picture of what's happening; in particular, we can see the call to __0_alloc_with_del____ which allocates the W_NDimArray for the result, and the raw_malloc which allocates the actual array. Then we have a long list of 149 simple operations which set the fields of the resulting array, construct an iterator, and finally do a call_assembler: this is the actual logic to do the addition, which was JITtted indipendently; call_assembler is one of the operations to do JIT-to-JIT calls:  All of this is very suboptimal: in this particular case, we know that the shape of self.matrix is always (3, 2): so, we are doing an incredible amount of work, including calling\u00a0malloc() twice for the temporary arrays, just to call two functions which ultimately do a total of 6 multiplications and 6 additions.  Note also that this is not a fault of the JIT: CPython+numpy has to do the same amount of work, just hidden inside C calls.  One possible solution to this nonsense is a well known compiler optimization: loop unrolling.  From the compiler point of view, unrolling the loop is always risky because if the matrix is too big you might end up emitting a huge blob of code, possibly uselss if the shape of the matrices change frequently: this is the main reason why the PyPy JIT does not even try to do it in this case.  However, we know that the matrix is small, and always of the same shape. So, let's unroll the loop manually: <pre>class SpecializedCreature(Creature):\n\n    def __init__(self, *args, **kwargs):\n        Creature.__init__(self, *args, **kwargs)\n        # store the data in a plain Python list\n        self.data = list(self.matrix.ravel()) + list(self.constant)\n        self.data_state = [0.0]\n        assert self.matrix.shape == (2, 3)\n        assert len(self.data) == 8\n\n    def run_step(self, inputs):\n        # state: [state_vars ... inputs]\n        # out_values: [state_vars, ... outputs]\n        k0, k1, k2, q0, q1, q2, c0, c1 = self.data\n        s0 = self.data_state[0]\n        z_sp, z = inputs\n        #\n        # compute the output\n        out0 = s0*k0 + z_sp*k1 + z*k2 + c0\n        out1 = s0*q0 + z_sp*q1 + z*q2 + c1\n        #\n        self.data_state[0] = out0\n        outputs = [out1]\n        return outputs\n</pre> In the actual code there is also a sanity check which asserts that the computed output is the very same as the one returned by Creature.run_step.  So, let's try to see how it performs. First, with CPython: <pre>$ python -m ev.main\nGeneration   1: ... [population = 500]  [7.61 secs]\nGeneration   2: ... [population = 500]  [3.96 secs]\nGeneration   3: ... [population = 500]  [3.79 secs]\nGeneration   4: ... [population = 500]  [3.74 secs]\nGeneration   5: ... [population = 500]  [3.84 secs]\nGeneration   6: ... [population = 500]  [3.69 secs]\n</pre> This looks good: 60% faster than the original CPython+numpy implementation. Let's try on PyPy: <pre>Generation   1: ... [population = 500]  [0.39 secs]\nGeneration   2: ... [population = 500]  [0.10 secs]\nGeneration   3: ... [population = 500]  [0.11 secs]\nGeneration   4: ... [population = 500]  [0.09 secs]\nGeneration   5: ... [population = 500]  [0.08 secs]\nGeneration   6: ... [population = 500]  [0.12 secs]\nGeneration   7: ... [population = 500]  [0.09 secs]\nGeneration   8: ... [population = 500]  [0.08 secs]\nGeneration   9: ... [population = 500]  [0.08 secs]\nGeneration  10: ... [population = 500]  [0.08 secs]\nGeneration  11: ... [population = 500]  [0.08 secs]\nGeneration  12: ... [population = 500]  [0.07 secs]\nGeneration  13: ... [population = 500]  [0.07 secs]\nGeneration  14: ... [population = 500]  [0.08 secs]\nGeneration  15: ... [population = 500]  [0.07 secs]\n</pre> Yes, it's not an error. After a couple of generations, it stabilizes at around ~0.07-0.08 seconds per generation. This is around 80 (eighty) times faster than the original CPython+numpy implementation, and around 35-40x faster than the naive PyPy+numpypy one.  Let's look at the trace again: it no longer contains expensive calls, and certainly no more temporary malloc() s. The core of the logic is between lines 386-416, where we can see that it does fast C-level multiplications and additions: float_mul and float_add are translated straight into mulsd and addsd x86 instructions.  As I said before, this is a very particular example, and the techniques described here do not always apply: it is not realistic to expect an 80x speedup on arbitrary code, unfortunately. However, it clearly shows the potential of PyPy when it comes to high-speed computing. And most importantly, it's not a toy benchmark which was designed specifically to have good performance on PyPy: it's a real world example, albeit small.  You might be also interested in the talk I gave at last EuroPython, in which I talk about a similar topic: \"The Joy of PyPy JIT: abstractions for free\" (abstract, slides and video).  How to reproduce the results <pre>$ git clone https://github.com/antocuni/evolvingcopter\n$ cd evolvingcopter\n$ {python,pypy} -m ev.main --no-specialized --no-numpypy\n$ {python,pypy} -m ev.main --no-specialized\n$ {python,pypy} -m ev.main\n</pre>","tags":["jit","profiling","speed","pypy"]},{"location":"2018/04/27/how-to-ignore-the-annoying-cython-warnings-in-pypy-60/","title":"How to ignore the annoying Cython warnings in PyPy 6.0","text":"<p>Originally published on the PyPy blog.</p>  If you install any Cython-based module in PyPy 6.0.0, it is very likely that you get a warning like this: <pre><code>&gt;&gt;&gt;&gt; import numpy\n/data/extra/pypy/6.0.0/site-packages/numpy/random/__init__.py:99: UserWarning: __builtin__.type size changed, may indicate binary incompatibility. Expected 888, got 408\n  from .mtrand import *\n</code></pre>  The TL;DR version is: the warning is a false alarm, and you can hide it by doing: <pre><code>$ pypy -m pip install pypy-fix-cython-warning\n</code></pre>  The package does not contain any module, only a\u00a0<code>.pth</code>\u00a0file which installs a warning filter at startup.  Technical details  This happens because whenever Cython compiles a pyx file, it generates C code which does a sanity check on the C size of\u00a0<code>PyType_Type</code>. PyPy versions up to 5.10 are buggy and report the incorrect size, so Cython includes a workaround to compare it with the incorrect value, when on PyPy.  PyPy 6 fixed the bug and now\u00a0<code>PyType_Type</code>\u00a0reports the correct size; however, Cython still tries to compare it with the old, buggy value, so it (wrongly) emits the warning.  Cython 0.28.2 includes a fix for it, so that C files generated by it no longer emit the warning. However, most packages are distributed with pre-cythonized C files. For example,\u00a0<code>numpy-1.14.2.zip</code>\u00a0include C files which were generated by Cython 0.26.1: if you compile it you still get the warning, even if you locally installed a newer version of Cython.  There is not much that we can do on the PyPy side, apart for waiting for all the Cython-based packages to do a new release which include C files generated by a newer Cython.\u00a0 In the mean time, installing this module will silence the\u00a0warning.","tags":["pypy"]},{"location":"2018/09/21/inside-cpyext-why-emulating-cpython-c-api-is-so-hard/","title":"Inside cpyext: Why emulating CPython C API is so Hard","text":"<p>Originally published on the PyPy blog.</p> cpyext is PyPy's subsystem which provides a compatibility layer to compile and run CPython C extensions inside PyPy.  Often people ask why a particular C extension doesn't work or is very slow on PyPy. Usually it is hard to answer without going into technical details. The goal of this blog post is to explain some of these technical details, so that we can simply link here instead of explaining again and again :). From a 10.000 foot view, cpyext is PyPy's version of \"Python.h\". Every time you compile an extension which uses that header file, you are using cpyext. This includes extension explicitly written in C (such as numpy) and extensions which are generated from other compilers/preprocessors (e.g. Cython). At the time of writing, the current status is that most C extensions \"just work\". Generally speaking, you can simply pip install them, provided they use the public, official C API instead of poking at private implementation details.  However, the performance of cpyext is generally poor. A Python program which makes heavy use of cpyext extensions is likely to be slower on PyPy than on CPython. Note: in this blog post we are talking about Python 2.7 because it is still the default version of PyPy: however most of the implementation of cpyext is shared with PyPy3, so everything applies to that as well.  C API Overview In CPython, which is written in C, Python objects are represented as PyObject*, i.e. (mostly) opaque pointers to some common \"base struct\". CPython uses a very simple memory management scheme: when you create an object, you allocate a block of memory of the appropriate size on the heap. Depending on the details, you might end up calling different allocators, but for the sake of simplicity, you can think that this ends up being a call to malloc(). The resulting block of memory is initialized and casted to to PyObject*: this address never changes during the object lifetime, and the C code can freely pass it around, store it inside containers, retrieve it later, etc. Memory is managed using reference counting. When you create a new reference to an object, or you discard a reference you own, you have to increment or decrement the reference counter accordingly. When the reference counter goes to 0, it means that the object is no longer used and can safely be destroyed. Again, we can simplify and say that this results in a call to free(), which finally releases the memory which was allocated by malloc(). Generally speaking, the only way to operate on a PyObject* is to call the appropriate API functions. For example, to convert a given PyObject* to a C integer, you can use PyInt_AsLong(); to add two objects together, you can call PyNumber_Add(). Internally, PyPy uses a similar approach. All Python objects are subclasses of the RPython W_Root class, and they are operated by calling methods on the space singleton, which represents the interpreter. At first, it looks very easy to write a compatibility layer: just make PyObject* an alias for W_Root, and write simple RPython functions (which will be translated to C by the RPython compiler) which call the space accordingly: <pre>def PyInt_AsLong(space, o):\n    return space.int_w(o)\n\ndef PyNumber_Add(space, o1, o2):\n    return space.add(o1, o2)\n</pre> Actually, the code above is not too far from the real implementation. However, there are tons of gory details which make it much harder than it looks, and much slower unless you pay a lot of attention to performance.  The PyPy GC To understand some of cpyext challenges, you need to have at least a rough idea of how the PyPy GC works. Contrarily to the popular belief, the \"Garbage Collector\" is not only about collecting garbage: instead, it is generally responsible for all memory management, including allocation and deallocation. Whereas CPython uses a combination of malloc/free/refcounting to manage memory, the PyPy GC uses a completely different approach. It is designed assuming that a dynamic language like Python behaves the following way: <ul> <li>You create, either directly or indirectly, lots of objects.</li> <li>Most of these objects are temporary and very short-lived. Think e.g. of doing a + b + c: you need to allocate an object to hold the temporary result of a + b, then it dies very quickly because you no longer need it when you do the final + c part.</li> <li>Only small fraction of the objects survive and stay around for a while.</li> </ul>  So, the strategy is: make allocation as fast as possible; make deallocation of short-lived objects as fast as possible; find a way to handle the remaining small set of objects which actually survive long enough to be important. This is done using a Generational GC: the basic idea is the following: <ol> <li>We have a nursery, where we allocate \"young objects\" very quickly.</li> <li>When the nursery is full, we start what we call a \"minor collection\".<ul> <li>We do a quick scan to determine the small set of objects which survived so far</li> <li>We move these objects out of the nursery, and we place them in the area of memory which contains the \"old objects\". Since the address of the objects changes, we fix all the references to them accordingly.</li> </ul> </li> </ol> <ol> <li>now the nursery contains only objects which \"died young\". We can discard all of them very quickly, reset the nursery, and use the same area of memory to allocate new objects from now.</li> </ol>  In practice, this scheme works very well and it is one of the reasons why PyPy is much faster than CPython.  However, careful readers have surely noticed that this is a problem for cpyext. On one hand, we have PyPy objects which can potentially move and change their underlying memory address; on the other hand, we need a way to represent them as fixed-address PyObject* when we pass them to C extensions.  We surely need a way to handle that. PyObject* in PyPy Another challenge is that sometimes, PyObject* structs are not completely opaque: there are parts of the public API which expose to the user specific fields of some concrete C struct. For example the definition of PyTypeObject which exposes many of the tp_* slots to the user. Since the low-level layout of PyPy W_Root objects is completely different than the one used by CPython, we cannot simply pass RPython objects to C; we need a way to handle the difference. So, we have two issues so far: objects can move, and incompatible low-level layouts. cpyext solves both by decoupling the RPython and the C representations. We have two \"views\" of the same entity, depending on whether we are in the PyPy world (the movable W_Root subclass) or in the C world (the non-movable PyObject*). PyObject* are created lazily, only when they are actually needed. The vast majority of PyPy objects are never passed to any C extension, so we don't pay any penalty in that case. However, the first time we pass a W_Root to C, we allocate and initialize its PyObject* counterpart. The same idea applies also to objects which are created in C, e.g. by calling PyObject_New(). At first, only the PyObject* exists and it is exclusively managed by reference counting. As soon as we pass it to the PyPy world (e.g. as a return value of a function call), we create its W_Root counterpart, which is managed by the GC as usual. Here we start to see why calling cpyext modules is more costly in PyPy than in CPython. We need to pay some penalty for all the conversions between W_Root and PyObject*. Moreover, the first time we pass a W_Root to C we also need to allocate the memory for the PyObject* using a slowish \"CPython-style\" memory allocator. In practice, for all the objects which are passed to C we pay more or less the same costs as CPython, thus effectively \"undoing\" the speedup guaranteed by PyPy's Generational GC under normal circumstances.  Maintaining the link between W_Root and PyObject* We now need a way to convert between W_Root and PyObject* and vice-versa; also, we need to to ensure that the lifetime of the two entities are in sync. In particular: <ol> <li>as long as the W_Root is kept alive by the GC, we want the PyObject* to live even if its refcount drops to 0;</li> <li>as long as the PyObject* has a refcount greater than 0, we want to make sure that the GC does not collect the W_Root.</li> </ol>  The PyObject* \u21e8 W_Root link is maintained by the special field ob_pypy_link which is added to all PyObject*. On a 64 bit machine this means that all PyObject* have 8 bytes of overhead, but then the conversion is very quick, just reading the field. For the other direction, we generally don't want to do the same: the assumption is that the vast majority of W_Root objects will never be passed to C, and adding an overhead of 8 bytes to all of them is a waste. Instead, in the general case the link is maintained by using a dictionary, where W_Root are the keys and PyObject* the values. However, for a few selected W_Root subclasses we do maintain a direct link using the special _cpy_ref field to improve performance. In particular, we use it for W_TypeObject (which is big anyway, so a 8 bytes overhead is negligible) and W_NoneObject. None is passed around very often, so we want to ensure that the conversion to PyObject* is very fast. Moreover it's a singleton, so the 8 bytes overhead is negligible as well. This means that in theory, passing an arbitrary Python object to C is potentially costly, because it involves doing a dictionary lookup.  We assume that this cost will eventually show up in the profiler: however, at the time of writing there are other parts of cpyext which are even more costly (as we will show later), so the cost of the dict lookup is never evident in the profiler.  Crossing the border between RPython and C There are two other things we need to care about whenever we cross the border between RPython and C, and vice-versa: exception handling and the GIL. In the C API, exceptions are raised by calling PyErr_SetString() (or one of many other functions which have a similar effect), which basically works by creating an exception value and storing it in some global variable. The function then signals that an exception has occurred by returning an error value, usually NULL. On the other hand, in the PyPy interpreter, exceptions are propagated by raising the RPython-level OperationError exception, which wraps the actual app-level exception values. To harmonize the two worlds, whenever we return from C to RPython, we need to check whether a C API exception was raised and if so turn it into an OperationError. We won't dig into details of how the GIL is handled in cpyext. For the purpose of this post, it is enough to know that whenever we enter C land, we store the current thread id into a global variable which is accessible also from C; conversely, whenever we go back from RPython to C, we restore this value to 0. Similarly, we need to do the inverse operations whenever you need to cross the border between C and RPython, e.g. by calling a Python callback from C code. All this complexity is automatically handled by the RPython function generic_cpy_call. If you look at the code you see that it takes care of 4 things: <ol> <li>Handling the GIL as explained above.</li> <li>Handling exceptions, if they are raised.</li> <li>Converting arguments from W_Root to PyObject*.</li> <li>Converting the return value from PyObject* to W_Root.</li> </ol>  So, we can see that calling C from RPython introduce some overhead. Can we measure it? Assuming that the conversion between W_Root and PyObject* has a reasonable cost (as explained by the previous section), the overhead introduced by a single border-cross is still acceptable, especially if the callee is doing some non-negligible amount of work. However this is not always the case. There are basically three problems that make (or used to make) cpyext super slow: <ol> <li>Paying the border-crossing cost for trivial operations which are called very often, such as Py_INCREF.</li> <li>Crossing the border back and forth many times, even if it's not strictly needed.</li> <li>Paying an excessive cost for argument and return value conversions.</li> </ol>  The next sections explain in more detail each of these problems.  Avoiding unnecessary roundtrips Prior to the 2017 Cape Town Sprint, cpyext was horribly slow, and we were well aware of it: the main reason was that we never really paid too much attention to performance. As explained in the blog post, emulating all the CPython quirks is basically a nightmare, so better to concentrate on correctness first. However, we didn't really know why it was so slow. We had theories and assumptions, usually pointing at the cost of conversions between W_Root and PyObject*, but we never actually measured it. So, we decided to write a set of cpyext microbenchmarks to measure the performance of various operations.  The result was somewhat surprising: the theory suggests that when you do a cpyext C call, you should pay the border-crossing costs only once, but what the profiler told us was that we were paying the cost of generic_cpy_call several times more than what we expected. After a bit of investigation, we discovered this was ultimately caused by our \"correctness-first\" approach. For simplicity of development and testing, when we started cpyext we wrote everything in RPython: thus, every single API call made from C (like the omnipresent PyArg_ParseTuple(), PyInt_AsLong(), etc.) had to cross back the C-to-RPython border. This was especially daunting for very simple and frequent operations like Py_INCREF and Py_DECREF, which CPython implements as a single assembly instruction! Another source of slow down was the implementation of PyTypeObject slots. At the C level, these are function pointers which the interpreter calls to do certain operations, e.g. tp_new to allocate a new instance of that type. As usual, we have some magic to implement slots in RPython; in particular, _make_wrapper does the opposite of generic_cpy_call: it takes a RPython function and wraps it into a C function which can be safely called from C, handling the GIL, exceptions and argument conversions automatically. This was very handy during the development of cpyext, but it might result in some bad nonsense; consider what happens when you call the following C function: <pre>static PyObject* foo(PyObject* self, PyObject* args)\n{\n    PyObject* result = PyInt_FromLong(1234);\n    return result;\n}\n</pre> <ol> <li>you are in RPython and do a cpyext call to foo: RPython-to-C;</li> <li>foo calls PyInt_FromLong(1234), which is implemented in RPython: C-to-RPython;</li> <li>the implementation of PyInt_FromLong indirectly calls PyIntType.tp_new, which is a C function pointer: RPython-to-C;</li> <li>however, tp_new is just a wrapper around an RPython function, created by _make_wrapper: C-to-RPython;</li> <li>finally, we create our RPython W_IntObject(1234); at some point during the RPython-to-C crossing, its PyObject* equivalent is created;</li> <li>after many layers of wrappers, we are again in foo: after we do return result, during the C-to-RPython step we convert it from PyObject* to W_IntObject(1234).</li> </ol> Phew! After we realized this, it was not so surprising that cpyext was very slow :). And this was a simplified example, since we are not passing a PyObject* to the API call. When we do, we need to convert it back and forth at every step.  Actually, I am not even sure that what I described was the exact sequence of steps which used to happen, but you get the general idea. The solution is simple: rewrite as much as we can in C instead of RPython, to avoid unnecessary roundtrips. This was the topic of most of the Cape Town sprint and resulted in the cpyext-avoid-roundtrip branch, which was eventually merged. Of course, it is not possible to move everything to C: there are still operations which need to be implemented in RPython. For example, think of PyList_Append: the logic to append an item to a list is complex and involves list strategies, so we cannot replicate it in C.  However, we discovered that a large subset of the C API can benefit from this. Moreover, the C API is huge. While we invented this new way of writing cpyext code, we still need to convert many of the functions to the new paradigm.  Sometimes the rewrite is not automatic or straighforward. cpyext is a delicate piece of software, so it happens often that we make a mistake and end up staring at a segfault in gdb. However, the most important takeaway is that the performance improvements we got from this optimization are impressive, as we will detail later.  Conversion costs The other potential big source of slowdown is the conversion of arguments between W_Root and PyObject*. As explained earlier, the first time you pass a W_Root to C, you need to allocate its PyObject* counterpart. Suppose you have a foo function defined in C, which takes a single int argument: <pre>for i in range(N):\n    foo(i)\n</pre> To run this code, you need to create a different PyObject* for each value of i: if implemented naively, it means calling N times malloc() and free(), which kills performance. CPython has the very same problem, which is solved by using a free list to allocate ints. So, what we did was to simply steal the code from CPython and do the exact same thing. This was also done in the cpyext-avoid-roundtrip branch, and the benchmarks show that it worked perfectly. Every type which is converted often to PyObject* must have a very fast allocator. At the moment of writing, PyPy uses free lists only for ints and tuples: one of the next steps on our TODO list is certainly to use this technique with more types, like float. Conversely, we also need to optimize the conversion from PyObject* to W_Root: this happens when an object is originally allocated in C and returned to Python. Consider for example the following code: <pre>import numpy as np\nmyarray = np.random.random(N)\nfor i in range(len(arr)):\n    myarray[i]\n</pre> At every iteration, we get an item out of the array: the return type is a an instance of numpy.float64 (a numpy scalar), i.e. a PyObject'*: this is something which is implemented by numpy entirely in C, so completely opaque to cpyext. We don't have any control on how it is allocated, managed, etc., and we can assume that allocation costs are the same as on CPython. As soon as we return these PyObject* to Python, we need to allocate their W_Root equivalent. If you do it in a small loop like in the example above, you end up allocating all these W_Root inside the nursery, which is a good thing since allocation is super fast (see the section above about the PyPy GC). However, we also need to keep track of the W_Root to PyObject* link. Currently, we do this by putting all of them in a dictionary, but it is very inefficient, especially because most of these objects die young and thus it is wasted work to do that for them.  Currently, this is one of the biggest unresolved problem in cpyext, and it is what causes the two microbenchmarks allocate_int and allocate_tuple to be very slow. We are well aware of the problem, and we have a plan for how to fix it. The explanation is too technical for the scope of this blog post as it requires a deep knowledge of the GC internals to be understood, but the details are here.  C API quirks Finally, there is another source of slowdown which is beyond our control. Some parts of the CPython C API are badly designed and expose some of the implementation details of CPython. The major example is reference counting. The Py_INCREF / Py_DECREF API is designed in such a way which forces other implementation to emulate refcounting even in presence of other GC management schemes, as explained above. Another example is borrowed references. There are API functions which do not incref an object before returning it, e.g. PyList_GetItem().  This is done for performance reasons because we can avoid a whole incref/decref pair, if the caller needs to handle the returned item only temporarily: the item is kept alive because it is in the list anyway. For PyPy, this is a challenge: thanks to list strategies, lists are often represented in a compact way. For example, a list containing only integers is stored as a C array of long.  How to implement PyList_GetItem? We cannot simply create a PyObject* on the fly, because the caller will never decref it and it will result in a memory leak. The current solution is very inefficient. The first time we do a PyList_GetItem, we convert the whole list to a list of PyObject*. This is bad in two ways: the first is that we potentially pay a lot of unneeded conversion cost in case we will never access the other items of the list. The second is that by doing that we lose all the performance benefit granted by the original list strategy, making it slower for the rest of the pure-python code which will manipulate the list later. PyList_GetItem is an example of a bad API because it assumes that the list is implemented as an array of PyObject*: after all, in order to return a borrowed reference, we need a reference to borrow, don't we? Fortunately, (some) CPython developers are aware of these problems, and there is an ongoing project to design a better C API which aims to fix exactly this kind of problem. Nonetheless, in the meantime we still need to implement the current half-broken APIs. There is no easy solution for that, and it is likely that we will always need to pay some performance penalty in order to implement them correctly. However, what we could potentially do is to provide alternative functions which do the same job but are more PyPy friendly: for example, we could think of implementing PyList_GetItemNonBorrowed or something like that: then, C extensions could choose to use it (possibly hidden inside some macro and #ifdef) if they want to be fast on PyPy.  Current performance During the whole blog post we claimed cpyext is slow. How slow it is, exactly? We decided to concentrate on microbenchmarks for now. It should be evident by now there are simply too many issues which can slow down a cpyext program, and microbenchmarks help us to concentrate on one (or few) at a time. The microbenchmarks measure very simple things, like calling functions and methods with the various calling conventions (no arguments, one arguments, multiple arguments); passing various types as arguments (to measure conversion costs); allocating objects from C, and so on. Here are the results from the old PyPy 5.8 relative and normalized to CPython 2.7, the lower the better:  PyPy was horribly slow everywhere, ranging from 2.5x to 10x slower. It is particularly interesting to compare simple.noargs, which measures the cost of calling an empty function with no arguments, and simple.onearg(i), which measures the cost calling an empty function passing an integer argument: the latter is ~2x slower than the former, indicating that the conversion cost of integers is huge. PyPy 5.8 was the last release before the famous Cape Town sprint, when we started to look at cpyext performance seriously. Here are the performance data for PyPy 6.0, the latest release at the time of writing:  The results are amazing! PyPy is now massively faster than before, and for most benchmarks it is even faster than CPython: yes, you read it correctly: PyPy is faster than CPython at doing CPython's job, even considering all the extra work it has to do to emulate the C API.  This happens thanks to the JIT, which produces speedups high enough to counterbalance the slowdown caused by cpyext. There are two microbenchmarks which are still slower though: allocate_int and allocate_tuple, for the reasons explained in the section about Conversion costs.  Next steps Despite the spectacular results we got so far, cpyext is still slow enough to kill performance in most real-world code which uses C extensions extensively (e.g., the omnipresent numpy). Our current approach is something along these lines: <ol> <li>run a real-world small benchmark which exercises cpyext</li> <li>measure and find the major bottleneck</li> <li>write a corresponding microbenchmark</li> <li>optimize it</li> <li>repeat</li> </ol>  On one hand, this is a daunting task because the C API is huge and we need to tackle functions one by one.  On the other hand, not all the functions are equally important, and is is enough to optimize a relatively small subset to improve many different use cases. Where a year ago we announced we have a working answer to run c-extension in PyPy, we now have a clear picture of what are the performance bottlenecks, and we have developed some technical solutions to fix them. It is \"only\" a matter of tackling them, one by one.  It is worth noting that most of the work was done during two sprints, for a total 2-3 person-months of work. We think this work is important for the Python ecosystem. PyPy has established a baseline for performance in pure python code, providing an answer for the \"Python is slow\" detractors. The techniques used to make cpyext performant will let PyPy become an alternative for people who mix C extensions with Python, which, it turns out, is just about everyone, in particular those using the various scientific libraries. Today, many developers are forced to seek performance by converting code from Python to a lower language. We feel there is no reason to do this, but in order to prove it we must be able to run both their python and their C extensions performantly, then we can begin to educate them how to write JIT-friendly code in the first place. We envision a future in which you can run arbitrary Python programs on PyPy, with the JIT speeding up the pure Python parts and the C parts running as fast as today: the best of both worlds!","tags":["cpyext","profiling","speed","pypy"]},{"location":"2019/01/03/pypy-for-low-latency-systems/","title":"PyPy for low-latency systems","text":"<p>Originally published on the PyPy blog.</p>  PyPy for low-latency systems Recently I have merged the gc-disable branch, introducing a couple of features which are useful when you need to respond to certain events with the lowest possible latency.  This work has been kindly sponsored by Gambit Research (which, by the way, is a very cool and geeky place where to work, in case you are interested).  Note also that this is a very specialized use case, so these features might not be useful for the average PyPy user, unless you have the same problems as described here.   The PyPy VM manages memory using a generational, moving Garbage Collector. Periodically, the GC scans the whole heap to find unreachable objects and frees the corresponding memory.  Although at a first look this strategy might sound expensive, in practice the total cost of memory management is far less than e.g. on CPython, which is based on reference counting.  While maybe counter-intuitive, the main advantage of a non-refcount strategy is that allocation is very fast (especially compared to malloc-based allocators), and deallocation of objects which die young is basically for free. More information about the PyPy GC is available here.  As we said, the total cost of memory managment is less on PyPy than on CPython, and it's one of the reasons why PyPy is so fast.  However, one big disadvantage is that while on CPython the cost of memory management is spread all over the execution of the program, on PyPy it is concentrated into GC runs, causing observable pauses which interrupt the execution of the user program. To avoid excessively long pauses, the PyPy GC has been using an incremental strategy since 2013. The GC runs as a series of \"steps\", letting the user program to progress between each step.  The following chart shows the behavior of a real-world, long-running process:  The orange line shows the total memory used by the program, which increases linearly while the program progresses. Every ~5 minutes, the GC kicks in and the memory usage drops from ~5.2GB to ~2.8GB (this ratio is controlled by the PYPY_GC_MAJOR_COLLECT env variable). The purple line shows aggregated data about the GC timing: the whole collection takes ~1400 individual steps over the course of ~1 minute: each point represent the maximum time a single step took during the past 10 seconds. Most steps take ~10-20 ms, although we see a horrible peak of ~100 ms towards the end. We have not investigated yet what it is caused by, but we suspect it is related to the deallocation of raw objects.  These multi-millesecond pauses are a problem for systems where it is important to respond to certain events with a latency which is both low and consistent. If the GC kicks in at the wrong time, it might causes unacceptable pauses during the collection cycle.  Let's look again at our real-world example. This is a system which continuously monitors an external stream; when a certain event occurs, we want to take an action. The following chart shows the maximum time it takes to complete one of such actions, aggregated every minute:  You can clearly see that the baseline response time is around ~20-30 ms. However, we can also see periodic spikes around ~50-100 ms, with peaks up to ~350-450 ms! After a bit of investigation, we concluded that most (although not all) of the spikes were caused by the GC kicking in at the wrong time.  The work I did in the gc-disable branch aims to fix this problem by introducing two new features to the gc module: <ul> <li>gc.disable(), which previously only inhibited the execution of finalizers without actually touching the GC, now disables the GC major collections. After a call to it, you will see the memory usage grow indefinitely.</li> <li>gc.collect_step() is a new function which you can use to manually execute a single incremental GC collection step.</li> </ul>  It is worth to specify that gc.disable() disables only the major collections, while minor collections still runs.  Moreover, thanks to the JIT's virtuals, many objects with a short and predictable lifetime are not allocated at all. The end result is that most objects with short lifetime are still collected as usual, so the impact of gc.disable() on memory growth is not as bad as it could sound.  Combining these two functions, it is possible to take control of the GC to make sure it runs only when it is acceptable to do so.  For an example of usage, you can look at the implementation of a custom GC inside pypytools. The peculiarity is that it also defines a \"with nogc():\" context manager which you can use to mark performance-critical sections where the GC is not allowed to run.  The following chart compares the behavior of the default PyPy GC and the new custom GC, after a careful placing of nogc() sections:  The yellow line is the same as before, while the purple line shows the new system: almost all spikes have gone, and the baseline performance is about 10% better. There is still one spike towards the end, but after some investigation we concluded that it was not caused by the GC.  Note that this does not mean that the whole program became magically faster: we simply moved the GC pauses in some other place which is not shown in the graph: in this specific use case this technique was useful because it allowed us to shift the GC work in places where pauses are more acceptable.  All in all, a pretty big success, I think.  These functionalities are already available in the nightly builds of PyPy, and will be included in the next release: take this as a New Year present :)  Antonio Cuni and the PyPy team","tags":["gc","sponsors","pypy"]},{"location":"2019/02/11/pypy-v700-triple-release-of-27-35-and-36-alpha/","title":"PyPy v7.0.0: triple release of 2.7, 3.5 and 3.6-alpha","text":"<p>Originally published on the PyPy blog.</p>  The PyPy team is proud to release the version 7.0.0 of PyPy, which includes three different interpreters: <ul> <li>PyPy2.7, which is an interpreter supporting the syntax and the features of Python 2.7</li> <li>PyPy3.5, which supports Python 3.5</li> <li>PyPy3.6-alpha: this is the first official release of PyPy to support 3.6 features, although it is still considered alpha quality.</li> </ul>  All the interpreters are based on much the same codebase, thus the triple release. Until we can work with downstream providers to distribute builds with PyPy, we have made packages for some common packages available as wheels. The GC hooks , which can be used to gain more insights into its performance, has been improved and it is now possible to manually manage the GC by using a combination of gc.disable and gc.collect_step. See the GC blog post. We updated the cffi module included in PyPy to version 1.12, and the cppyy backend to 1.4. Please use these to wrap your C and C++ code, respectively, for a JIT friendly experience. As always, this release is 100% compatible with the previous one and fixed several issues and bugs raised by the growing community of PyPy users. We strongly recommend updating. The PyPy3.6 release and the Windows PyPy3.5 release are still not production quality so your mileage may vary. There are open issues with incomplete compatibility and c-extension support. The utf8 branch that changes internal representation of unicode to utf8 did not make it into the release, so there is still more goodness coming. You can download the v7.0 releases here: https://pypy.org/download.html We would like to thank our donors for the continued support of the PyPy project. If PyPy is not quite good enough for your needs, we are available for direct consulting work. We would also like to thank our contributors and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython's JIT even better.  What is PyPy? PyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7, 3.5 and 3.6. It's fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler. We also welcome developers of other dynamic languages to see what RPython can do for them. The PyPy release supports: <ul> <li>x86 machines on most common operating systems (Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)</li> <li>big- and little-endian variants of PPC64 running Linux,</li> <li>s390x running Linux</li> </ul>  Unfortunately at the moment of writing our ARM buildbots are out of service, so for now we are not releasing any binary for the ARM architecture.  What else is new? PyPy 6.0 was released in April, 2018. There are many incremental improvements to RPython and PyPy, the complete listing is here.  Please update, and continue to help us make PyPy better.  Cheers, The PyPy team","tags":["release","pypy"]},{"location":"2019/12/18/hpy-kick-off-sprint-report/","title":"HPy kick-off sprint report","text":"<p>Originally published on the PyPy blog.</p> <p>Recently Antonio, Armin and Ronan had a small internal sprint in the beautiful city of Gda\u0144sk to kick-off the development of HPy. Here is a brief report of what was accomplished during the sprint. </p> What is HPy? <p>The TL;DR answer is \"a better way to write C extensions for Python\".</p> <p>The idea of HPy was born during EuroPython 2019 in Basel, where there was an informal meeting which included core developers of PyPy, CPython (Victor Stinner and Mark Shannon) and Cython (Stefan Behnel). The ideas were later also discussed with Tim Felgentreff of GraalPython, to make sure they would also be applicable to this very different implementation, Windel Bouwman of RustPython is following the project as well.</p> <p>All of us agreed that the current design of the CPython C API is problematic for various reasons and, in particular, because it is too tied to the current internal design of CPython.  The end result is that:</p> <ul> <li>alternative implementations of Python (such as PyPy, but not only) have a hard time loading and executing existing C extensions;</li> <li>CPython itself is unable to change some of its internal implementation details without breaking the world. For example, as of today it would be impossible to switch from using reference counting to using a real GC, which in turns make it hard for example to remove the GIL, as gilectomy attempted.</li> </ul> <p>HPy tries to address these issues by following two major design guidelines:</p> <ol> <li>objects are referenced and passed around using opaque handles, which are similar to e.g., file descriptors in spirit. Multiple, different handles can point to the same underlying object, handles can be duplicated and each handle must be released independently of any other duplicate.</li> <li>The internal data structures and C-level layout of objects are not visible nor accessible using the API, so each implementation if free to use what fits best.</li> </ol> <p>The other major design goal of HPy is to allow incremental transition and porting, so existing modules can migrate their codebase one method at a time. Moreover, Cython is considering to optionally generate HPy code, so extension module written in Cython would be able to benefit from HPy automatically.</p> <p>More details can be found in the README of the official HPy repository.</p> Target ABI <p>When compiling an HPy extension you can choose one of two different target ABIs:</p> <ul> <li>HPy/CPython ABI: in this case, hpy.h contains a set of macros and static inline functions. At compilation time this translates the HPy API into the standard C-API. The compiled module will have no performance penalty, and it will have a \"standard\" filename like foo.cpython-37m-x86_64-linux-gnu.so.</li> <li>Universal HPy ABI: as the name implies, extension modules compiled this way are \"universal\" and can be loaded unmodified by multiple Python interpreters and versions.  Moreover, it will be possible to dynamically enable a special debug mode which will make it easy to find e.g., open handles or memory leaks, without having to recompile the extension.</li> </ul> <p>Universal modules can also be loaded on CPython, thanks to the hpy_universal module which is under development. An extra layer of indirection enables loading extensions compiled with the universal ABI. Users of hpy_universal will face a small performance penalty compared to the ones using the HPy/CPython ABI.</p> <p>This setup gives several benefits:</p> <ul> <li>Extension developers can use the extra debug features given by the Universal ABI with no need to use a special debug version of Python.</li> <li>Projects which need the maximum level of performance can compile their extension for each relevant version of CPython, as they are doing now.</li> <li>Projects for which runtime speed is less important will have the choice of distributing a single binary which will work on any version and implementation of Python.</li> </ul> A simple example <p>The HPy repo contains a proof of concept module. Here is a simplified version which illustrates what a HPy module looks like:</p> <pre>\n#include \"hpy.h\"\n\nHPy_DEF_METH_VARARGS(add_ints)\nstatic HPy add_ints_impl(HPyContext ctx, HPy self, HPy *args, HPy_ssize_t nargs)\n{\n    long a, b;\n    if (!HPyArg_Parse(ctx, args, nargs, \"ll\", &amp;a, &amp;b))\n        return HPy_NULL;\n    return HPyLong_FromLong(ctx, a+b);\n}\n\n\nstatic HPyMethodDef PofMethods[] = {\n    {\"add_ints\", add_ints, HPy_METH_VARARGS, \"\"},\n    {NULL, NULL, 0, NULL}\n};\n\nstatic HPyModuleDef moduledef = {\n    HPyModuleDef_HEAD_INIT,\n    .m_name = \"pof\",\n    .m_doc = \"HPy Proof of Concept\",\n    .m_size = -1,\n    .m_methods = PofMethods\n};\n\n\nHPy_MODINIT(pof)\nstatic HPy init_pof_impl(HPyContext ctx)\n{\n    HPy m;\n    m = HPyModule_Create(ctx, &amp;moduledef);\n    if (HPy_IsNull(m))\n        return HPy_NULL;\n    return m;\n}\n</pre> <p>People who are familiar with the current C-API will surely notice many similarities. The biggest differences are:</p> <ul> <li>Instead of PyObject *, objects have the type HPy, which as explained above represents a handle.</li> <li>You need to explicitly pass an HPyContext around: the intent is primary to be future-proof and make it easier to implement things like sub- interpreters.</li> <li>HPy_METH_VARARGS is implemented differently than CPython's METH_VARARGS: in particular, these methods receive an array of HPy and its length, instead of a fully constructed tuple: passing a tuple makes sense on CPython where you have it anyway, but it might be an unnecessary burden for alternate implementations.  Note that this is similar to the new METH_FASTCALL which was introduced in CPython.</li> <li>HPy relies a lot on C macros, which most of the time are needed to support the HPy/CPython ABI compilation mode. For example, HPy_DEF_METH_VARARGS expands into a trampoline which has the correct C signature that CPython expects (i.e., PyObject (*)(PyObject *self, *PyObject *args)) and which calls add_ints_impl.</li> </ul> Sprint report and current status <p>After this long preamble, here is a rough list of what we accomplished during the week-long sprint and the days immediatly after.</p> <p>On the HPy side, we kicked-off the code in the repo: at the moment of writing the layout of the directories is a bit messy because we moved things around several times, but we identified several main sections:</p> <ol> <li><p>A specification of the API which serves both as documentation and as an input for parts of the projects which are automatically generated. Currently, this lives in public_api.h.</p> </li> <li><p>A set of header files which can be used to compile extension modules: depending on whether the flag -DHPY_UNIVERSAL_ABI is passed to the compiler, the extension can target the HPy/CPython ABI or the HPy Universal ABI</p> </li> <li><p>A CPython extension module called hpy_universal which makes it possible to import universal modules on CPython</p> </li> <li><p>A set of tests which are independent of the implementation and are meant to be an \"executable specification\" of the semantics.  Currently, these tests are run against three different implementations of the HPy API:</p> <ul> <li>the headers which implements the \"HPy/CPython ABI\"</li> <li>the hpy_universal module for CPython</li> <li>the hpy_universal module for PyPy (these tests are run in the PyPy repo)</li> </ul> </li> </ol> <p>Moreover, we started a PyPy branch in which to implement the hpy_univeral module: at the moment of writing PyPy can pass all the HPy tests apart the ones which allow conversion to and from PyObject *. Among the other things, this means that it is already possible to load the very same binary module in both CPython and PyPy, which is impressive on its own :).</p> <p>Finally, we wanted a real-life use case to show how to port a module to HPy and to do benchmarks.  After some searching, we choose ultrajson, for the following reasons:</p> <ul> <li>it is a real-world extension module which was written with performance in mind</li> <li>when parsing a JSON file it does a lot of calls to the Python API to construct the various parts of the result message</li> <li>it uses only a small subset of the Python API</li> </ul> <p>This repo contains the HPy port of ultrajson. This commit shows an example of what the porting looks like.</p> <p>ujson_hpy is also a very good example of incremental migration: so far only ujson.loads is implemented using the HPy API, while ujson.dumps is still implemented using the old C-API, and both can coexist nicely in the same compiled module.</p> Benchmarks <p>Once we have a fully working ujson_hpy module, we can finally run benchmarks!  We tested several different versions of the module:</p> <ul> <li>ujson: this is the vanilla implementation of ultrajson using the C-API. On PyPy this is executed by the infamous cpyext compatibility layer, so we expect it to be much slower than on CPython</li> <li>ujson_hpy: our HPy port compiled to target the HPy/CPython ABI. We expect it to be as fast as ujson</li> <li>ujson_hpy_universal: same as above but compiled to target the Universal HPy ABI. We expect it to be slightly slower than ujson on CPython, and much faster on PyPy.</li> </ul> <p>Finally, we also ran the benchmark using the builtin json module. This is not really relevant to HPy, but it might still be an interesting as a reference data point.</p> <p>The benchmark is very simple and consists of parsing a big JSON file 100 times. Here is the average time per iteration (in milliseconds) using the various versions of the module, CPython 3.7 and the latest version of the hpy PyPy branch:</p> CPython PyPy ujson 154.32 633.97 ujson_hpy 152.19 ujson_hpy_universal 168.78 207.68 json 224.59 135.43 <p>As expected, the benchmark proves that when targeting the HPy/CPython ABI, HPy doesn't impose any performance penalty on CPython. The universal version is ~10% slower on CPython, but gives an impressive 3x speedup on PyPy! It it worth noting that the PyPy hpy module is not fully optimized yet, and we expect to be able to reach the same performance as CPython for this particular example (or even more, thanks to our better GC).</p> <p>All in all, not a bad result for two weeks of intense hacking :)</p> <p>It is also worth noting than PyPy's builtin json module does really well in this benchmark, thanks to the recent optimizations that were described in an earlier blog post.</p> Conclusion and future directions <p>We think we can be very satisfied about what we have got so far. The development of HPy is quite new, but these early results seem to indicate that we are on the right track to bring Python extensions into the future.</p> <p>At the moment, we can anticipate some of the next steps in the development of HPy:</p> <ul> <li>Think about a proper API design: what we have done so far has been a \"dumb\" translation of the API we needed to run ujson. However, one of the declared goal of HPy is to improve the design of the API. There will be a trade-off between the desire of having a clean, fresh new API and the need to be not too different than the old one, to make porting easier.  Finding the sweet spot will not be easy!</li> <li>Implement the \"debug\" mode, which will help developers to find bugs such as leaking handles or using invalid handles.</li> <li>Instruct Cython to emit HPy code on request.</li> <li>Eventually, we will also want to try to port parts of numpy to HPy to finally solve the long-standing problem of sub-optimal numpy performance in PyPy.</li> </ul> <p>Stay tuned!</p>","tags":["hpy"]},{"location":"2021/03/29/hello-hpy/","title":"Hello, HPy","text":"<p>Originally published on the HPy blog.</p> <p>HPy has been around for a while now. The initial discussion started during EuroPython 2019, in the good old times when we could still go to conferences and have real-life meetings. Since then, HPy progressed a lot from the point of view of the actual code, but we have been a bit too silent w.r.t. communicating what we are doing to the external world and to the broader Python community. Hopefully, now that this blog is online we will do a better job at periodically communicating the status of HPy, so make sure to subscribe to the RSS feed.</p>","tags":["hpy"]},{"location":"2021/03/29/hello-hpy/#what-is-hpy","title":"What is HPy?","text":"<p>Quoting the frontpage of our website:</p> <p>HPy provides a new API for extending Python in C. In other words, you use <code>#include &lt;hpy.h&gt;</code> instead of <code>#include &lt;Python.h&gt;</code>.</p> <p>The official Python/C API is specific to the current implementation of CPython: it exposes many internal details which makes it hard:</p> <ul> <li> <p>to implement it for other Python implementations (e.g. PyPy, GraalPython,   Jython, IronPython, etc.)</p> </li> <li> <p>to experiment with new things inside CPython itself: e.g. using a GC   instead of refcounting, or to remove the GIL.</p> </li> <li>to correctly check things like refcount handling: the external API gets mixed in with implementation details that should be hidden.</li> </ul> <p>Over the years, it has become evident that emulating the Python/C API in an efficient way is challenging, if not impossible. The main goal of HPy is expose a C API which is possible to implement in an efficient way on a number of very diverse Python implementations.</p> <p>There are several advantages to writing your C extension in HPy:</p> <ul> <li> <p>it runs much faster on PyPy, and at native speed on CPython</p> </li> <li> <p>it is possible to compile a single binary which runs unmodified on all   supported Python implementations and versions</p> </li> <li> <p>it is simpler and more manageable than the Python/C API</p> </li> <li> <p>it provides an improved debugging experience: in \"debug mode\", HPy   actively checks for many common mistakes such as reference leaks and   invalid usage of objects after they have been deleted. It is possible to   turn the \"debug mode\" on at startup time, without needing to recompile   Python or the extension itself</p> </li> </ul> <p>See also the official docs for a more in-depth overview.</p>","tags":["hpy"]},{"location":"2021/03/29/hello-hpy/#show-me-an-example","title":"Show me an example","text":"<p>This is a \"normal\" Python/C extension:</p> <p>{{% listing 2021/03/hello-hpy/hello_old.c c %}}</p> <p>We can compile it with a simple <code>setup.py</code>:</p> <pre><code>from setuptools import setup, Extension\nsetup(\n    name=\"hello\",\n    ext_modules = [\n        Extension('hello_old', ['hello_old.c']),\n    ],\n)\n</code></pre> <pre><code>$ python setup.py build_ext --inplace\n...\n$ python\n&gt;&gt;&gt; import hello_old\n&gt;&gt;&gt; hello_old.add(10, 20)\n30\n</code></pre> <p>Now, let's start to uncover the HPy equivalent, piece by piece:</p> <pre><code>#include &lt;hpy.h&gt;\n\nHPyDef_METH(add, \"add\", add_impl, HPyFunc_VARARGS,\n            .doc = \"add two integers\");\n\nstatic HPy add_impl(HPyContext ctx, HPy self, HPy *args, HPy_ssize_t nargs) {\n    long a, b;\n    if (!HPyArg_Parse(ctx, NULL, args, nargs, \"ll\", &amp;a, &amp;b))\n        return HPy_NULL;\n    return HPyLong_FromLong(ctx, a+b);\n}\n</code></pre> <p>There are a bunch of things which are different from the usual C-extension module:</p> <ul> <li> <p>the former <code>PyObject *</code> is now <code>HPy</code>, which we call \"a handle\". Handles are   similar to <code>PyObject *</code>, but are completely opaque: for more information,   see the   official docs.</p> </li> <li> <p>There is an additional parameter, <code>HPyContext ctx</code>. One of the problems of   the old API is that often it implicitly relies on the existence of a   per-thread or per-subinterpreter local state. <code>HPyContext</code> makes this state explicit. This   makes the whole API more regular and makes it possible to develop new   interesting features such as the   Universal ABI   and the Debug mode.</p> </li> <li> <p>HPy introduces the concept of <code>HPyDef</code>s. <code>HPyDef_METH</code> is a macro which   generates the definition of an <code>HPyDef</code> static constant named <code>add</code>, which   represents the definition of a Python method implemented by the C function   <code>add_impl</code>. In this specific example <code>HPyDef_METH</code> contains more or less the   same informations as the old <code>PyMethodDef</code>, but <code>HPyDef</code> are more   general. For example, when defining custom types you can use things like   <code>HPyDef_SLOT</code>, <code>HPyDef_GETSET</code>, etc.</p> </li> <li> <p>Notice that we no longer need the cast to <code>(PyCFunction)</code>. One of the   biggest advantages of <code>HPyDef_METH</code> is that since it's a macro, it can   automatically generate a forward declaration for <code>add_impl</code>, with the   correct signature. This means that if you use the wrong number and/or type   of parameters, you get a nice compile-time error instead of an obscure crash   at runtime.</p> </li> <li> <p>The signature corresponding to <code>HPyFunc_VARARGS</code> is slighly different than   the old <code>METH_VARARGS</code>: we pass positional arguments as a C array instead of   a Python tuple. This means that it is possible to call the function without   having to allocate a Python tuple, and for example the PyPy implementation   of HPy takes advantage of that. This is very similar to CPython's   VectorCall protocol.</p> </li> </ul> <p>Note</p> <p>In this post, we are using a slightly old version of HPy. If you try with a newer version you should use <code>HPyContext *ctx</code> instead of <code>HPyContext ctx</code>. See also Issue #150 and PR #182.</p> <p>Let's continue our tour of <code>hello_new.c</code>:</p> <pre><code>static HPyDef *hello_defines[] = {\n    &amp;add,\n    NULL\n};\n\nstatic HPyModuleDef moduledef = {\n    HPyModuleDef_HEAD_INIT,\n    .m_name = \"hello_new\",\n    .m_doc = \"hello example using the new HPy API\",\n    .m_size = -1,\n    .defines = hello_defines,\n};\n\nHPy_MODINIT(hello_new)\nstatic HPy init_hello_new_impl(HPyContext ctx) {\n    return HPyModule_Create(ctx, &amp;moduledef);\n}\n</code></pre> <p>This is pretty similar to the old code. The biggest change is that instead of declaring an array of <code>PyMethodDef</code>, we create an array of <code>HPyDef</code> as discussed above.</p> <p>Finally, we need to modify <code>setup.py</code>. Compiling an HPy extension is as easy as adding <code>setup_requires=['hpy.devel']</code> and use <code>hpy_ext_modules</code>:</p> <p>{{% listing 2021/03/hello-hpy/setup.py python %}}</p>","tags":["hpy"]},{"location":"2021/03/29/hello-hpy/#compiling-hpy-extensions","title":"Compiling HPy extensions","text":"<p>In this demo, we will show how to setup an environment to try HPy and compile extensions on both CPython and PyPy.</p> <p>At the moment HPy is still in its early stages and the API is still subject to change, so we have not done any official release yet. For the same reason, if you want to use HPy on PyPy or GraalPython, you need to manually ensure to install a version which is supported. This is just temporary, and this kind of things will be sorted out automatically once we start to roll out official releases.</p> <p>So, we need to install HPy from the github repo. Moreover, the HPy implementations inside PyPy and GraalPython are lagging behind a little, so we will install a slightly old revision:</p> <ul> <li> <p>HPy revision eb07982</p> </li> <li> <p>nightly build of the PyPy hpy branch: a2f7c80062e8 for linux64</p> </li> <li> <p>PyPy nighly builds: main page and   hpy branch</p> </li> <li> <p>GraalPython nightly build: Linux or macOS</p> </li> <li> <p>source code of this example</p> </li> </ul> <p>The first step is to create a <code>venv</code> for CPython and install <code>hpy</code>:</p> <pre><code>$ python3 -m venv tryhpy\n$ . tryhpy/bin/activate\n$ pip install wheel\n$ pip install git+git://github.com/hpyproject/hpy.git@eb07982\n</code></pre> <p>To install a nightly build of PyPy it is enough to unpack the tarball and run <code>-m ensurepip</code>. We can check what is the HPy version supported by PyPy by calling <code>hpy.universal.get_version()</code>:</p> <pre><code>$ curl -O http://buildbot.pypy.org/nightly/hpy/pypy-c-jit-101860-a2f7c80062e8-linux64.tar.bz2\n$ tar xf pypy-c-jit-101860-a2f7c80062e8-linux64.tar.bz2\n$ ./pypy-c-jit-101860-a2f7c80062e8-linux64/bin/pypy -m ensurepip\n\n$ ./pypy-c-jit-101860-a2f7c80062e8-linux64/bin/pypy\n&gt;&gt;&gt;&gt; import hpy.universal\n&gt;&gt;&gt;&gt; hpy.universal.get_version()\n('0.1.dev959+geb07982', 'eb07982')\n</code></pre> <p>For GraalPython, just unpack the tarball and create a venv:</p> <pre><code>$ curl -LO https://github.com/graalvm/graalvm-ce-dev-builds/releases/download/21.1.0-dev-20210330_0726/graalpython-dev-linux-amd64.tar.gz\n$ tar xzf graalpython-dev-linux-amd64.tar.gz\n$ graalpython-21.1.0-dev-linux-amd64/bin/graalpython -m venv hpy-venv\n$ hpy-venv/bin/graalpython\n&gt;&gt;&gt; import hpy.universal\n&gt;&gt;&gt; hpy.universal.get_version()\n&gt;&gt;&gt; ('0.1.dev950+g98f448a', '98f448a')\n</code></pre> <p>Now that our enviroment is ready, we can compile and try our extensions:</p> <pre><code>$ cd /path/to/example/\n$ . /path/to/tryhpy/bin/activate    # activate the venv\n$ python setup.py build_ext --inplace\n[...]\n\n$ ls -1 *.so\nhello_new.cpython-38-x86_64-linux-gnu.so\nhello_old.cpython-38-x86_64-linux-gnu.so\n\n$ python\n&gt;&gt;&gt; import hello_old, hello_new\n&gt;&gt;&gt; hello_old.add(10, 20)\n30\n&gt;&gt;&gt; hello_new.add(30, 40)\n70\n&gt;&gt;&gt;\n</code></pre> <p>It worked! One important thing to note is the filename of <code>hello_new</code>: <code>.cpython-38-x86_64-linux-gnu.so</code> is the standard filename for CPython 3.8 extension modules. This happens because by default <code>hpy_ext_modules</code> targets the CPython ABI. As such, from the point of view of CPython <code>hello_new</code> is indistinguishable from <code>hello_old</code>. It also means that HPy is required only to compile it but not to import it later. Finally, we expect the performance to be the very same as the extensions using the old API.</p> <p>However, we can also explicitly ask HPy to produce an \"universal binary\", which targets the HPy Universal ABI: as the name implies, universal binaries can be imported by CPython, but also by alternative implementations such as PyPy. We can build universal binaries by passing <code>--hpy-abi=universal</code> to <code>setup.py</code>:</p> <pre><code>$ # clean the previous build\n$ rm -rf build/ *.so\n\n$ python setup.py --hpy-abi=universal build_ext --inplace\n$ ls -1 *.so\nhello_new.hpy.so\nhello_old.cpython-38-x86_64-linux-gnu.so\n</code></pre> <p>Note the filename: <code>hello_old</code> is still a CPython-specific extension, but <code>hello_new.hpy.so</code> is an universal binary. Once compiled, you can import it as usual:</p> <pre><code>$ python\n&gt;&gt;&gt; import hello_old, hello_new\n&gt;&gt;&gt; hello_old.add(10, 20)\n30\n&gt;&gt;&gt; hello_new.add(30, 40)\n70\n&gt;&gt;&gt; hello_new.__file__\n'/.../hello-hpy/hello_new.hpy.so'\n</code></pre> <p>Note</p> <p>At the moment of writing, because of Issue #191 if you try to print the repr of <code>hello_new</code>, you see something like this (note the <code>.py</code> extension):</p> <pre><code>&gt;&gt;&gt; hello_new\n&lt;module 'hello_new' from '/.../hello-hpy/hello_new.py'&gt;\n</code></pre> <p>Note that on its own, CPython does not know how to import <code>.hpy.so</code> files. The magic is done by the <code>hello_new.py</code>, which is automatically generated by <code>setup.py</code>:</p> <pre><code>$ cat hello_new.py\n[...]\ndef __bootstrap__():\n    [...]\n    from hpy.universal import load_from_spec\n    ext_filepath = pkg_resources.resource_filename(__name__, 'hello_new.hpy.so')\n    m = load_from_spec(Spec('hello_new', ext_filepath))\n    [...]\n    sys.modules[__name__] = m\n\n__bootstrap__()\n</code></pre> <p>Finally, we can try to import our shiny new universal binary on PyPy:</p> <pre><code>$ /path/to/pypy-c-jit-101860-a2f7c80062e8-linux64/bin/pypy\n&gt;&gt;&gt;&gt; import hello_new\n&gt;&gt;&gt;&gt; hello_new.add(10, 20)\n30\n&gt;&gt;&gt;&gt; hello_new.__file__\n'/.../hello-hpy/hello_new.hpy.so'\n&gt;&gt;&gt;&gt;\n</code></pre> <p>Similarly, it also just works on GraalPython:</p> <pre><code>$ /path/to/graalpython/hpy-venv/bin/graalpython\n&gt;&gt;&gt; import hello_new\n&gt;&gt;&gt; hello_new.add(10, 20)\n30\n</code></pre> <p>That's all you need to get started with HPy. What we presented today is just the basics, of course: in the next posts we will dig more into the technical details, and show more interesting features than just a hello world.</p> <p>Stay tuned!</p> <p>(edited on 2021-03-31 to include GraalPython)</p>","tags":["hpy"]},{"location":"2021/07/15/hpy-002-first-public-release/","title":"hpy 0.0.2: First public release","text":"<p>Originally published on the HPy blog.</p> <p>HPy 0.0.2 is out! This is the first version which is officially released and made available on PyPI.</p> <p>The major highlight of this release is that it is supported by three different Python implementations: CPython, PyPy and GraalPython.</p>","tags":["hpy"]},{"location":"2021/07/15/hpy-002-first-public-release/#what-is-hpy","title":"What is HPy?","text":"<p>HPy provides a new API for extending Python in C. In other words, you use <code>#include &lt;hpy.h&gt;</code> instead of <code>#include &lt;Python.h&gt;</code>. For more info, look at the official documentation.</p>","tags":["hpy"]},{"location":"2021/07/15/hpy-002-first-public-release/#installation","title":"Installation","text":"<p>HPy 0.0.2 only supports Linux systems, and it's only tested on <code>x86_64</code>. Windows support is already present on master, and it will be included in the next release.</p> <p>For CPython, you need to install it manually, using pip:</p> <pre><code>$ pip install hpy==0.0.2\n</code></pre> <p>Note</p> <p>Currently, we provide only the <code>sdist</code> (i.e., the <code>.tar.gz</code>, no binary wheels). See also issue #223, contributions are welcome :).</p> <p>PyPy and GraalPython ships their own version of HPy, so no installation is necessary. HPy 0.0.2 will be included in the next release of both, i.e. PyPy 7.3.6 (expected in October 2021) and GraalPython 21.2.0 (expected on 2021-07-20). In the meantime, you can download a nightly build:</p> <ul> <li> <p>PyPy 3.7 nightly builds, for example revision 3bf99c09018b</p> </li> <li> <p>GraalPython: download the latest <code>graalpython-dev</code> package from   this page</p> </li> </ul> <p>To double check the version of HPy which is shipped with those, you can either use <code>pip</code> or <code>hpy.universal.get_version()</code>:</p> <pre><code>$ pypy -m pip show hpy\nName: hpy\nVersion: 0.0.2\nSummary: A better C API for Python\nHome-page: https://hpyproject.org\nAuthor: The HPy team\nAuthor-email: hpy-dev@python.org\nLicense: MIT\n...\n\n$ pypy -c 'import hpy.universal; print(hpy.universal.get_version()[0])'\n0.0.2\n</code></pre>","tags":["hpy"]},{"location":"2021/07/15/hpy-002-first-public-release/#api","title":"API","text":"<p>At the moment HPy supports only a small fraction of the full API offered by the old Python/C API, but it is enough to write non-trivial extensions, and the documentation is scarce. <code>public_api.h</code>, which is used to autogenerate parts of the HPy code, is a reliable list of all the supported functions.</p> <p>Warning</p> <p>The HPy API is still considered in alpha status and it's subject to change between versions. In fact, the current master is already incompatible with hpy-0.0.2 because of PR #182, which renamed all occurences of <code>HPyContext</code> into <code>HPyContext*</code>.</p>","tags":["hpy"]},{"location":"2021/07/15/hpy-002-first-public-release/#examples","title":"Examples","text":"<p>The best way to get a glimpse of how to use HPy is to look at examples:</p> <ul> <li> <p>the HPy repository contains a   \"proof of concept\" package.   Make sure to checkout the branch <code>release/0.0.2</code>.</p> </li> <li> <p><code>ultrajson-hpy</code>   is a port of the popular <code>ultrajson</code> package. Make sure to checkout the   <code>hpy-0.0.2</code> branch.</p> </li> <li> <p><code>piconumpy</code>   contains a very tiny implementation of an <code>array</code>-like class. Make sure to   checkout the <code>hpy-0.0.2</code> branch.</p> </li> </ul>","tags":["hpy"]},{"location":"2021/05/12/hpy--python-language-summit/","title":"HPy @ Python Language Summit","text":"<p>Originally published on the HPy blog.</p> <p>Yesterday I had the privilege to give a talk about HPy (sildes) at the Python Language Summit 2021.</p> <p>The organizers of the summit will soon publish a full report about the event (edit: now available here), but for the HPy-specific part, we got generally good feedback. Someone has a few concerns that if CPython is to change the API, HPy might not be going far enough. Others said that Python shouldn't wait for the \"perfect\" API if HPy can be the \"good\" one that helps it evolve.</p> <p>Everyone was open to have HPy-compatible wheels on PyPI, once the HPy Universal ABI stays relatively stable. Many people suggested that we should really write a PEP to propose HPy as a \"semi-official\" API for Python.</p> <p>An interesting question was about which are the VM optimizations which are compatible with the HPy API. The following is a non-exhaustive list of things which are known to work because they already used by PyPy and/or GraalPython:</p> <ul> <li>JIT compiler</li> <li>moving/compacting GCs</li> <li>storage strategies</li> <li>maps (also known as \"hidden classes\")</li> </ul> <p>One notable missing optimization from the list above is tagged pointers. Currently there is no implementation which uses tagged pointers and supports HPy. However, we don't think there is any fundamental design issue in HPy which would prevent it: if you turn tagged pointers into \"tagged handles\", things should just work out of the box.</p>","tags":["hpy"]},{"location":"2025/02/26/over-the-clouds-cpython-pyodide-and-spy/","title":"Over the clouds: CPython, Pyodide and SPy","text":"<p>The Python community is awesome.</p> <p>It is full of great people and minds, and interacting with people at conferences is always nice and stimulating. But one of my favorite things is that over time, after many conferences, talks, pull requests and beers, the personal relationship with some of them strengthen and they become friends.</p> <p></p> <p>I am fortunate enough that two of them, \u0141ukasz Langa and Hood Chatham, accepted my invitation to join me in Cervinia, at the border between Italian and Swiss Alps, for a week of hacking, winter sports and going literally over the clouds. This is a brief summary of what we did during our time together.</p>","tags":["spy","pyodide","cpython"]},{"location":"2025/02/26/over-the-clouds-cpython-pyodide-and-spy/#about-us","title":"About us","text":"<p>\u0141ukasz doesn't need much introduction, as he's one of the most visible personalities of the Python world: among other things, he has been the release manager of CPython 3.8 and 3.9, he is the creator of Black and these days is the CPython developer in residence.</p> <p>Hood is mainly known for his work on Pyodide, which in my opinion is one of the most underrated projects in the Python world: it allows us to run CPython in the browser by compiling CPython and huge sets of extension modules to WebAssembly, using Emscripten. This may sound easier than it is, because WebAssembly is a very weird and young platform, meaning that over the years the Pyodide maintainers have had to develop a considerable amount of patches to CPython itself, Emscripten, LLVM, etc.  If you use any website or project which allows you to run Python in the browser such as PyScript, there is a good chance it's actually Pyodide under the hood (pun intended \ud83d\ude05).</p> <p>As for me, like many other Pythonistas, my name is associated with many projects starting or ending (or both!) with \"Py\", like PyPy, HPy and PyScript.  A couple of years ago I decided that all these Pys weren't enough, so I started SPy, which is an experiment to see whether we can come up with a Python variant which can be easily interpreted (for good development experience) and compiled (for performance).  This is the appropriate place to give a big thanks to Anaconda, which is allowing me to work on it full time currently.</p>","tags":["spy","pyodide","cpython"]},{"location":"2025/02/26/over-the-clouds-cpython-pyodide-and-spy/#hacking","title":"Hacking","text":"<p>There is one thing which face-to-face pairing does and which is impossible to achieve with async and remote collaboration: you can see all the little tricks and tools which other people use in their daily hacking. On the first day, \u0141ukasz showed me the wonder of Xonsh, a multi platform shell written and scriptable in Python.</p> <p></p> <p>Likewise, very soon he noticed that whenever I pressed <code>TAB</code> at my Python REPL, I'd get colored completions, thanks to fancycompleter. This is a project which I started ~15 years ago and I've used since then: at that time, it couldn't work out of the box on CPython, because it required a patched version of <code>libreadline</code>: but nowadays CPython ships with pyrepl which does support colored completions out of the box: with that in mind, we thought that it could be a good idea to integrate it in CPython. The result of this work ended up in PR #130473: it is still very WIP but hopefully I'll be able to continue working on it in the next days or weeks.</p> <p>Meanwhile, Hood discovered that the latest version of Pyodide didn't work on iOS. In perfect accordance to the spirit of the week, \u0141ukasz promptly paired with him to fix the issue.</p> <p>So, with this, we have PRs for two of our three projects. SPy is next in line, but it deserves its own section.</p>","tags":["spy","pyodide","cpython"]},{"location":"2025/02/26/over-the-clouds-cpython-pyodide-and-spy/#the-first-spy-program-ever","title":"The first SPy program ever \ud83e\udd78","text":"<p>SPy occupied a significant portion of our week. At the beginning of the week, we dedicated time to explaining the fundamental concepts and design decisions to \u0141ukasz and Hood.</p> <p>Quoting what I wrote above:</p> <p>SPy is an experiment to see whether we can come up with a Python variant which can be easily interpreted (for good development experience) and compiled (for performance).</p> <p>Currently, the documentation is very scarce. The best source to understand the ideas behind it are probably the slides and recording of the talk which I gave at PyCon and EuroPython.</p> <p>Now, this is a perfect time for a big disclaimer:</p> <p>Warning</p> <p>SPy is in super early stage, not even alpha quality. It probably contains lots of bugs, the language design is not fully stabilized, many basic features are probably missing.</p> <p>That said, SPy can already do interesting things. In particular, after I showed the array example, \u0141ukasz realized that despite the immaturity, SPy is already good enough to speed up one of his generative art projects.</p> <p>He has already written an extensive post about it, so I'm not going to repeat the full story here. Let me just quote a few intriguing excerpts to pique your interest in reading more:</p> <p> </p> <p>Let\u2019s get one thing out of the way. SPy is a research project in its early stages at the moment. You should not attempt to use it yet, unless you plan to contribute to it, and even then you have to come with the right mindset. [...]</p> <p>With all this in mind, SPy looks very attractive to me already. It\u2019s a language designed to be friendly to Python users, but is not attempting to be Python-compatible. It can\u2019t be, because with SPy, user code can be fully compiled to native binaries or WebAssembly.</p> <p>For the first end-user project in SPy, I decided to convert an existing Genuary entry I made with PyScript that draws an endless abstract topographic map. [...]</p> <p>This computation was too much for pure Python in either Pyodide or MicroPython to happen inside the animation loop, so in the original project I pre-computed the map area in a Web worker. [...]</p> <p>The SPy version of the project ditches the Web worker as the computation is over 100X faster. You\u2019re literally waiting longer for the background audio file to load. The result looks exactly the same, which was an important metric for us.</p> <p>Remember when we said that SPy still misses many basic features? Here is a list of PRs that \u0141ukasz had to make in order to achieve his goals:</p> <ul> <li>Add modulo operator for i32 #122</li> <li>Add bitwise operators for i32 #123</li> <li>Add post mortem to other exception types if --pdb was passed #124</li> <li>Teach the C compiler about f64_to_i32 conversions #125</li> </ul> <p>Thanks to this, we also got \u0141ukasz as a contributor to SPy. Only Hood was left...</p>","tags":["spy","pyodide","cpython"]},{"location":"2025/02/26/over-the-clouds-cpython-pyodide-and-spy/#spy-playground","title":"SPy playground","text":"<p>The SPy interpreter is written in Python (for now... eventually, it will be written in SPy itself), and Pyodide/PyScript makes it very easy to run Python in the browser. The goal for Hood and me was to be able to run the SPy interpreter on top of Pyodide, to make it easier for people to try it out.</p> <p>This proved to be challenging because of the very peculiar way in which SPy relies on WASM.  WebAssembly plays a central role in SPy, for two reasons:</p> <ul> <li> <p>compilation to .wasm is a first class feature (by converting .spy to .c and     then invoking clang)</p> </li> <li> <p>the interpreter uses wasmtime as a sandbox for     SPy's application-level memory allocation</p> </li> </ul> <p>The latter point requires some extra explanation: SPy includes a special \"unsafe\" mode that allows the use of low-level constructs like pointers and structs. These constructs, while powerful, pose a risk of crashing the interpreter due to their unsafe nature. To mitigate this risk, SPy executes these unsafe portions within a WASM sandbox using <code>wasmtime</code>. This approach ensures that any potential crashes are contained within the sandbox, preserving the stability of the interpreter. Additionally, this system is employed to call the runtime library, which is partially written in C. The C code is compiled to WASM and subsequently loaded by <code>wasmtime</code>, providing a secure and efficient execution environment.</p> <p>Another nice aspect of this architecture is that you can instantiate multiple SPy VMs in the same process, since all the state is stored in the sandoxed WASM memory.</p> <p>The challenging part is that <code>wasmtime</code> doesn't run on top of Pyodide. On the other hand, Pyodide literally sits on top of another WASM engine, provided by the browser and exposed by Emscripten.</p> <p>With this in mind, Hood and I started a SPy branch called pyodide: the plan was to create a layer called llwasm to abstract away the differences between <code>wasmtime</code> and Emscriptem, so that we could transparently use one or the other from the SPyVM.</p> <p>This proved to be more challenging than expected, in part because the WASM API exposed by Javascript/Emscripten is async, while the one exposed by wasmtime is sync.  Anyway, after a few days of intense work we managed to have it working \ud83c\udf89, although the PR is not merged yet because it requires some polishing.</p> <p>With that, I could hack together a quick &amp; dirty SPy playground: it is written with PyScript + LTK.  My web design skills leave a bit to be desired, so improvements and PRs are totally welcome. You can open it in an separate page to have it full screen, or you can load the inline version below:</p>       Load SPy Playground       <p>The playground allows to see the effect of all the various passes of the SPy pipeline:</p> <ol> <li><code>--execute</code>: run the code in the SPy interpreter</li> <li><code>--parse</code>: parse the code and dump the AST</li> <li><code>--redshift</code>: perform redshifting</li> <li><code>--cwrite</code>: convert redshifted code into C</li> </ol> <p>There is the last missing step: compiling the generated C code to WASM, which is currently not possible because it would require to run clang in the browser. Again, if anybody has any idea on how to make it happen, PRs are welcome.</p>","tags":["spy","pyodide","cpython"]},{"location":"2025/02/26/over-the-clouds-cpython-pyodide-and-spy/#conclusion","title":"Conclusion","text":"<p>It has been a fun and productive week! While this post is rich in technical details, I think it's important to highlight the value of personal relationships and the joy of spending time together. A big thank you to \u0141ukasz and Hood for visiting!</p>","tags":["spy","pyodide","cpython"]},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2021/","title":"2021","text":""},{"location":"archive/2019/","title":"2019","text":""},{"location":"archive/2018/","title":"2018","text":""},{"location":"archive/2017/","title":"2017","text":""},{"location":"archive/2012/","title":"2012","text":""},{"location":"archive/2011/","title":"2011","text":""},{"location":"archive/2010/","title":"2010","text":""},{"location":"archive/2009/","title":"2009","text":""},{"location":"archive/2008/","title":"2008","text":""},{"location":"category/post/","title":"Post","text":""},{"location":"page/2/","title":"Blog","text":""},{"location":"page/3/","title":"Blog","text":""},{"location":"page/4/","title":"Blog","text":""},{"location":"page/5/","title":"Blog","text":""},{"location":"category/post/page/2/","title":"Post","text":""},{"location":"category/post/page/3/","title":"Post","text":""},{"location":"category/post/page/4/","title":"Post","text":""},{"location":"category/post/page/5/","title":"Post","text":""}]}